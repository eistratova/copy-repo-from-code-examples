{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## improve performance - use smaller network/ different network;\n",
    "#                  add dropout; reduce no. of dropout instances (?); reduce dropout value\n",
    "#               remove kernel_initializer\n",
    "\n",
    "\n",
    "# add performance metrics other than accuracy\n",
    "# how to obtain the probabilities on test images, when using DataGenerator? \n",
    "\n",
    "## add colab; & check for all classes \n",
    "\n",
    "# transfer learning with images saved as RGB - also look up image size to be used\n",
    "\n",
    "# sparseConvNet; check kaggle code  (# check kaggle kernels for ~eco-taxa data)\n",
    "# loading image data from single file instead of directory; check on colab\n",
    "\n",
    "\n",
    "\n",
    "# https://machinelearningmastery.com/image-augmentation-deep-learning-keras/\n",
    "# https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# settle on no. of classes : confusion understanding taxonomic tree(?) - file inventory.txt\n",
    "#   see what was used in paper (108, excluding 12 rare taxa) - check if classes reported (see kaggle data ****)\n",
    "#                      & reported accuracy (>90%, recall: 40%, precision: 84%)\n",
    "## SEE SUPPLEMENTARY !!!\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "# EcoTaxa data: classification of Plankton\n",
    "# data source: https://www.seanoe.org/data/00446/55741/\n",
    "# http://marine-imaging-workshop.com/documents/miw17/presentations/public/MIW17_Poster_MarcPicheral.pdf\n",
    "\n",
    "\n",
    "## total no. of images (EcoTaxa data): 1433278  (~1.4 million)\n",
    "# the paper talks of 24M images\n",
    "\n",
    "###########################################################################################\n",
    "### PROBLEM STATEMENT ###\n",
    "# 1. build a classifier to classify images\n",
    "# 2. see if the classifier can point out 'new' classes, not present in the training data (class, junk, other) ****\n",
    "#     finding new classes :\n",
    "#           i) see if probability is less for all the known classes\n",
    "#           ii) use some similarity metric to the other classes (e.g. triplet loss?) ?\n",
    "# 3. comparison of features performance: conventional vs deep-learning based ***\n",
    "\n",
    "\n",
    "#### papers ####\n",
    "#https://www.frontiersin.org/articles/10.3389/fmars.2019.00196/full  (review, Apr. 2019) -> understand features used **\n",
    "\n",
    "\n",
    "#########################################################################################\n",
    "#### CHALLENGES: ####\n",
    "\n",
    "# 1. substantial re-training of pre-trained networks (transfer learning)\n",
    "#   or new network for gray-scale images?\n",
    "#          -> find image size corresponding to the pre-trained network used\n",
    "## TRY BOTH?\n",
    "# what happens if we i) pad the images vs ii) don't pad the images\n",
    "\n",
    "\n",
    "\n",
    "# 2. design of model & loss function suited to task  \n",
    "# https://www.kaggle.com/iafoss/pretrained-resnet34-with-rgby-0-460-public-lb\n",
    "\n",
    "\n",
    "\n",
    "# 3.  check data set size; handling class imbalance for image data (check for imbalance) ***\n",
    "\n",
    "# combine classes in similar taxa -> low within-class variance  (check images for grouped classes - are they similar?)\n",
    "# different classes have different no. of examples - use taxonomic tree to group\n",
    "\n",
    "#    check total no. of classes, depth of each class (can classify at shallower level using taxonomic tree)\n",
    "# select the level to do the classification at (this decides the no. of classes)\n",
    "\n",
    "#         --> try with 2-3 classes first\n",
    "\n",
    "# augment data (crop, translate, rotate, flip)\n",
    "# HOW TO HANDLE LARGE IMBALANCE AMONG DIFFERENT CLASSES - IS AUGMENTATION ENOUGH? OR REDUCE NO. OF IMAGES SELECTED?\n",
    "#  Luo et al, 2018 talks of different works - 4-class classification, 47 classes, 114 classes\n",
    "\n",
    "# - all classes should be represented at each batch - how to ensure this?\n",
    "\n",
    "# this should be an N-class classifier (instead of level-wise one-vs-rest)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4. handling outlier data?\n",
    "\n",
    "\n",
    "# 5. are all images of same size? are they captured on same scale?  ***\n",
    "# are all images of same size (height & width)? -> no\n",
    "# are they captured on same scale (magnification)? -> no idea\n",
    "# use Fiji/ ImageJ to check metadata of image (use voxel size? any other parameter) ****\n",
    "\n",
    "\n",
    "\n",
    "# 6. handling large no. of classes (classification performance metric); one-hot encode classes - which to use?\n",
    "# LabelEncoder() ; OneHotEncoder() ; LabelBinarizer() ; to_categorical (keras.utils)\n",
    "# https://stackoverflow.com/questions/50473381/scikit-learns-labelbinarizer-vs-onehotencoder\n",
    "# GOOGLE: label binarizer vs to categorical\n",
    "\n",
    "\n",
    "\n",
    "# 7. libraries to load & see image data\n",
    "# PIL/ Pillow vs openCV - which one to use?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 8. data pre-processing: is it necessary to create separate folders for train & test? ***\n",
    "# either create separate folders with the pre-processed images (suitable for large no. of images), \n",
    "# or\n",
    "# save the pre-processed images as numpy arrays and load them for quick processing (suitable for less no. of images,\n",
    "# or smaller images).\n",
    "# before pre-processing, shuffle the images. From here, train, test & val splits should be created.\n",
    "\n",
    "# In the former case, use ImageDataGenerator & flow_from_directory commands to load the images\n",
    "# (both scaled; with augmentation for training & without aug. for validation) & automatically read classes\n",
    "# in 2nd case, save the labels also as a separate file.\n",
    "\n",
    "\n",
    "\n",
    "# 9. train-test data split\n",
    "# ideally, use train, validation & test sets\n",
    "\n",
    "\n",
    "# 10. input shape (flatten image?); see how it should be fed into network ***\n",
    "#     (k x k x C, where k: image dimension, C: np. of channels)\n",
    "# see channel first (theano) vs channel last (tensorflow) conventions\n",
    "# also, the batch of images may sometimes be mentioned\n",
    "\n",
    "# how should the image info be saved & passed to the cnn for training? -> save flattened pixel values as features in csv file\n",
    "## re-shaping tha data for input to conv net\n",
    "# https://stackoverflow.com/questions/43235531/convolutional-neural-network-conv1d-input-shape\n",
    "# http://localhost:8888/notebooks/Desktop/data/HEALTHCARE/EEG/code/GPU_run_CNN/EEG_Classification_CNN_3patients_GPU.ipynb\n",
    "\n",
    "\n",
    "\n",
    "# 11. handle overfitting , use of batches (batch normalization) & epochs,\n",
    "#       early stopping,\n",
    "#       data augmentation (different techniques: rotate, shift, zoom, crop, etc)\n",
    "#      dropout regularization (different values)\n",
    "\n",
    "# 12. choice of activation function (ReLU, softmax, etc),\n",
    "#     loss function (MSE, categorical_crossentropy, etc), \n",
    "#     metric (acc., recall)\n",
    "\n",
    "# 13. hyper-parameter tuning: learning rate (else, adaptive learning rate like Adam), optimizer (Adam, SGD, etc)\n",
    "\n",
    "# 14. how to apply scaling to intermediate layers\n",
    "\n",
    "\n",
    "## is there a method to assess what is the best model that can be built from given data, in terms of metric values ?\n",
    "## (metric values: RMSD for regresion, precision/recall for classification)\n",
    "\n",
    "\n",
    "\n",
    "## activation function & vanishing gradient\n",
    "## deep vs wide network (see excel: how no. of parameters changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RESOURCES\n",
    "# sentdex youtube\n",
    "# data camp\n",
    "# pyimage\n",
    "# machinelearningmastery\n",
    "\n",
    "# https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-to-classify-photos-of-dogs-and-cats/  \n",
    "#     -> both array & file system load   \n",
    "# https://machinelearningmastery.com/start-here/#better   ****\n",
    "# https://machinelearningmastery.com/how-to-calculate-precision-recall-f1-and-more-for-deep-learning-models/\n",
    "# https://machinelearningmastery.com/custom-metrics-deep-learning-keras-python/\n",
    "# https://machinelearningmastery.com/image-augmentation-deep-learning-keras/\n",
    "# https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/\n",
    "\n",
    "\n",
    "# https://keras.io/preprocessing/image/       <- ImageDataGenerator\n",
    "# https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "\n",
    "\n",
    "# https://towardsdatascience.com/image-detection-from-scratch-in-keras-f314872006c9  -> gc command to clean memory\n",
    "\n",
    "# https://www.pyimagesearch.com/2018/09/10/keras-tutorial-how-to-get-started-with-keras-deep-learning-and-python/  -> from file\n",
    "\n",
    "# https://www.analyticsvidhya.com/blog/2019/01/build-image-classification-model-10-minutes/  -> Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output layer is fully connected layer (model.add(Dense(n, ...)))\n",
    "# where n=1 for regression;  or,   n=no. of classes \n",
    "# for binary classifier, should it be 1 with sigmoid (machinelearningmastery), or 2 with softmax activation (datacamp) ?\n",
    "# accordingly, under model.compile, loss should be binary_crossentropy or categorical_crossentropy (Classification)\n",
    "#                                                mean_squared_error for regression\n",
    "\n",
    "\n",
    "# for classification, should the labels be one-hot encoded?\n",
    "# how to assign labels for multi-class classification\n",
    "\n",
    "\n",
    "# input shape is specified in the first model.add layer\n",
    "# images can be fed as numpy array, or from separate train test folders\n",
    "# image pixels should be scaled\n",
    "# apply transform for data augmentation, if required\n",
    "\n",
    "\n",
    "# drop-out can be implemented after conv+pool block, & after fully connected (dense) block (except for last prediction layer) \n",
    "# learning rate is specified with optimizer under model.compile\n",
    "# no. of epochs are specified in model.fit (increase epochs if both train & test loss continue to show downward trend w/o plateau)\n",
    "# early stopping is specified in model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# colab:  https://colab.research.google.com/notebooks/io.ipynb#scrollTo=7taylj9wpsA2\n",
    "\n",
    "# any code to automate the download of data to google colab, & run codes there?  ***\n",
    "# https://www.analyticsvidhya.com/blog/2019/01/build-image-classification-model-10-minutes/  -> Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Colab\n",
    "!pip install PyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import os\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create a drive variable to access Google Drive:\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To download the dataset, we will use the ID of the file uploaded on Google Drive\n",
    "# Replace the id and filename in the below codes\n",
    "download = drive.CreateFile({'id': '1ZCzHDAfwgLdQke_GNnHp_4OheRRtNPs-'})\n",
    "download.GetContentFile('Train_UQcUa52.zip')\n",
    "!unzip Train_UQcUa52.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)  # prints location of executable\n",
    "print('')\n",
    "print(sys.version)   # prints python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "import gc  # garbage collector\n",
    "\n",
    "from random import seed\n",
    "from random import random\n",
    "# seed random number generator\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image  # Utilities to read and write images in various formats.\n",
    "import imageio     ## are both imageio & Pillow (PIL) required ?\n",
    "#import pathlib   # alternate to os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''link2 = Link[~Link[\"Lineage\"].str.contains('|'.join(['not-living', '/living/other/']))]\n",
    "\n",
    "# not living -> has 2 or 3 levels\n",
    "# living/other -> has 3 levels\n",
    "# living/eukaryote/... -> min 6 levels,max 15 levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADMAAAAwCAAAAACZXh1YAAADGElEQVR4nI2VW3PbOAyFDwhQ8kWW47YzO7P//89t0yTWJZbEG/ZBtiJLrls+cUh8xOEhCZLiSVO6dl7TP1+j5hkCwrjkGXE2+oTRkQIA0uLvGJp6dTDl3zEfza1XMjWziWf7cW/t2Gk0HuYCnvhW3wRp4+X4d3l8dVs4fZ8hTxnRZnTvg+v5+EPmqpcIo3un4TDfwkPm6nKZ/HX9bGY8IPPl5xMAELkEgLPK3cw8DwFAW31pl1QDAJu50w+0xYTz1PcKbX+FRf4Vc0xdN7lgEqhglPchskAaEG+aw7iyEtUaYBcxyzwHiEU/3q4qxJSUaZFmlQeJAiFd96MwxMs0D/bDQnxFRBA1O/yRwV5VAVR48zEEkoDJkt9pQw0dDGBqDV45YAeAnjNtZDVDxV6c04x4u3Tg4ftpXLApUegCON9as19eqbU2HFA3Ps9SCNZotMUqYPLgffbiyzwP7Xv9+ZmQUjsX3gLtTFtlEIihFE1Aq+fK0S7b7zmzxnEUIiCAiJS/tB0B1D5GS+AhXapzx0MhGcEnUhuj1aT2uN6PsRQdS+f6t//OPZ/Eosgyx5yQHevTQw8KoPKuc/5cta+9NQeVTfaC8TlOpi98q4eIOHz2zbn3Ll0+yzBsa1PcHareMU30LgUfgsox5r3vukGFl4dKN+ZNjSB470PykIzz/kXDxnjnwrw4EqA35lUuJECESKCYq0oRqNvk25z8u0yZxuIlAKB10NxGFxKZRBtvu139YuzFbDd5JiEEOn3luXpAkvd5TABJxkliu2tyln1Q2VgR9c58cFQjBc18++wGZ9QQW+aX2muMKEMSBhlVNs6E4C95Xtx57Vzas9hvAFD28CK2FDMdSt17HyNP53N+wa/OmHxrvo0O+uS9ZJZnJpfle5dcX42/A9UpSNLObq/Pvm376MxO9zcl14pcXWgj4xApmgOA9vZMPs69Qg7Zd6wKeIuJuW8/f3bIf/yLJ21VRxPblO+eIau/5JB0MBmv/5XfMGNYzrpdVqf7tq47H4n19Ch0aus6esrBDwKfMijSugw+ZxTVH9Lgf3/lhfGamGH3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=51x48 at 0x1C79AF3C588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# upload the data & check some images; \n",
    "\n",
    "'''im_path = pathlib.Path('./57398/ZooScanSet/Imgs/Acantharea/42738931.jpg')  #.glob('*/images/*.png')\n",
    "display(Image.open(im_path))  \n",
    "# https://stackoverflow.com/questions/11854847/how-can-i-display-an-image-from-a-file-in-jupyter-notebook\n",
    "# how to open an image in jupyter notebook\n",
    "# https://www.quora.com/How-do-I-load-multiple-images-in-Jupyter-Notebook-for-a-neural-network-classifier\n",
    "\n",
    "\n",
    "#training_paths = pathlib.Path('../input/stage1_train').glob('*/images/*.png')\n",
    "#training_sorted = sorted([x for x in training_paths])\n",
    "#im_path = training_sorted[45]\n",
    "#im = imageio.imread(str(im_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1433278"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total no. of images\n",
    "'''len(path_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-sizing images\n",
    "#https://stackoverflow.com/questions/44231209/resize-rectangular-image-to-square-keeping-ratio-and-fill-background-with-black/44231784\n",
    "# https://stackoverflow.com/questions/273946/how-do-i-resize-an-image-using-pil-and-maintain-its-aspect-ratio\n",
    "\n",
    "# as we go deeper into the network, the image gets smaller; hence can't start with very small images\n",
    "# see size to be used for pre-trained network (if) chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the images to square & make (increase or decrease) a common size\n",
    "\n",
    "#from PIL import Image\n",
    "'''\n",
    "def make_square(im, min_size=256, fill_color=(0, 0, 0, 0)):\n",
    "    x, y = im.size\n",
    "    size = max(min_size, x, y)\n",
    "    new_im = Image.new('RGBA', (size, size), fill_color)\n",
    "    new_im.paste(im, (int((size - x) / 2), int((size - y) / 2)))\n",
    "    return new_im\n",
    "'''\n",
    "\n",
    "def make_square(im, min_size=200, fill_color=(255)):   #fill_color=(255, 255, 255, 0)  # fill_color=(255, 255, 255)\n",
    "    # fill_color=(0, 0, 0, 0) <- black\n",
    "    # fill_color=(255, 255, 255, 0) <- white\n",
    "    x, y = im.size\n",
    "    \n",
    "    # make square image,\n",
    "    if x != y:\n",
    "        size = max(x, y)\n",
    "        new_im = Image.new('L', (size, size), fill_color)   # Image.new('RGB',  #'RGBA' is not supported in jpg image\n",
    "        new_im.paste(im, (int((size - x) / 2), int((size - y) / 2)))\n",
    "        #imResize = new_im.resize((min_size,min_size), Image.ANTIALIAS)\n",
    "    else:\n",
    "        new_im = im\n",
    "        # resize images\n",
    "        #imResize = im.resize((min_size,min_size), Image.ANTIALIAS)\n",
    "           \n",
    "    return new_im\n",
    "\n",
    "\n",
    "def im_resize(im, min_size=200):\n",
    "    # https://www.daniweb.com/programming/software-development/code/216637/resize-an-image-python\n",
    "    x, y = im.size\n",
    "    \n",
    "    if x > min_size:  # downsample\n",
    "        imResize = im.resize((min_size,min_size), Image.ANTIALIAS)\n",
    "    elif x < min_size: # upsample\n",
    "        imResize = im.resize((min_size, min_size), Image.BILINEAR)\n",
    "    else:\n",
    "        imResize = im\n",
    "    \n",
    "    return imResize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>path</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57398\\ZooScanSet\\Imgs\\Acantharea\\42738931.jpg</td>\n",
       "      <td>51</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>57398\\ZooScanSet\\Imgs\\Acantharea\\42745471.jpg</td>\n",
       "      <td>62</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1                                           path  \\\n",
       "0           0             0  57398\\ZooScanSet\\Imgs\\Acantharea\\42738931.jpg   \n",
       "1           1             1  57398\\ZooScanSet\\Imgs\\Acantharea\\42745471.jpg   \n",
       "\n",
       "   width  height  \n",
       "0     51      48  \n",
       "1     62      64  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do a trial run with selected folders -> Acantharea, Copilia, cypris\n",
    "\n",
    "trial = pd.read_csv(\"C:/Users/DAR9KOR/Desktop/data/sample_datasets/EcoTaxa/57398/selected_files.csv\")\n",
    "trial.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADMAAAAwCAAAAACZXh1YAAADGElEQVR4nI2VW3PbOAyFDwhQ8kWW47YzO7P//89t0yTWJZbEG/ZBtiJLrls+cUh8xOEhCZLiSVO6dl7TP1+j5hkCwrjkGXE2+oTRkQIA0uLvGJp6dTDl3zEfza1XMjWziWf7cW/t2Gk0HuYCnvhW3wRp4+X4d3l8dVs4fZ8hTxnRZnTvg+v5+EPmqpcIo3un4TDfwkPm6nKZ/HX9bGY8IPPl5xMAELkEgLPK3cw8DwFAW31pl1QDAJu50w+0xYTz1PcKbX+FRf4Vc0xdN7lgEqhglPchskAaEG+aw7iyEtUaYBcxyzwHiEU/3q4qxJSUaZFmlQeJAiFd96MwxMs0D/bDQnxFRBA1O/yRwV5VAVR48zEEkoDJkt9pQw0dDGBqDV45YAeAnjNtZDVDxV6c04x4u3Tg4ftpXLApUegCON9as19eqbU2HFA3Ps9SCNZotMUqYPLgffbiyzwP7Xv9+ZmQUjsX3gLtTFtlEIihFE1Aq+fK0S7b7zmzxnEUIiCAiJS/tB0B1D5GS+AhXapzx0MhGcEnUhuj1aT2uN6PsRQdS+f6t//OPZ/Eosgyx5yQHevTQw8KoPKuc/5cta+9NQeVTfaC8TlOpi98q4eIOHz2zbn3Ll0+yzBsa1PcHareMU30LgUfgsox5r3vukGFl4dKN+ZNjSB470PykIzz/kXDxnjnwrw4EqA35lUuJECESKCYq0oRqNvk25z8u0yZxuIlAKB10NxGFxKZRBtvu139YuzFbDd5JiEEOn3luXpAkvd5TABJxkliu2tyln1Q2VgR9c58cFQjBc18++wGZ9QQW+aX2muMKEMSBhlVNs6E4C95Xtx57Vzas9hvAFD28CK2FDMdSt17HyNP53N+wa/OmHxrvo0O+uS9ZJZnJpfle5dcX42/A9UpSNLObq/Pvm376MxO9zcl14pcXWgj4xApmgOA9vZMPs69Qg7Zd6wKeIuJuW8/f3bIf/yLJ21VRxPblO+eIau/5JB0MBmv/5XfMGNYzrpdVqf7tq47H4n19Ch0aus6esrBDwKfMijSugw+ZxTVH9Lgf3/lhfGamGH3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=51x48 at 0x1766ACA07B8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.open(str('./' + list(trial.path)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'42738931.jpg'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(trial.path)[0].split(\"Acantharea\\\\\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear memory\n",
    "# https://towardsdatascience.com/image-detection-from-scratch-in-keras-f314872006c9\n",
    "del trial\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data pre-processing: is it necessary to create separate folders for train & test?\n",
    "# https://cs230-stanford.github.io/train-dev-test-split.html  -> check for split using image indices \n",
    "\n",
    "#               total images        train(90%)     test(10%)     Total\n",
    "# Acantharea :   762                    686           76\n",
    "# Copilia:       762                    686           76\n",
    "# cypris:        702                    632           70\n",
    "# TOTAL                                2004          222         2226\n",
    "\n",
    "\n",
    "from os import makedirs\n",
    "from os import listdir\n",
    "from shutil import copyfile\n",
    "from random import seed\n",
    "from random import random\n",
    "\n",
    "# create directories\n",
    "dataset_home = './57398/ZooScanSet/runData/'\n",
    "subdirs = ['train/', 'test/']\n",
    "for subdir in subdirs:\n",
    "    # create label subdirectories\n",
    "    labeldirs = ['Acantharea/', 'Copilia/', 'cypris/']\n",
    "    for labldir in labeldirs:\n",
    "        newdir = dataset_home + subdir + labldir\n",
    "        makedirs(newdir, exist_ok=True)\n",
    "\n",
    "# seed random number generator\n",
    "seed(1)\n",
    "\n",
    "# define ratio of pictures to use for validation\n",
    "val_ratio = 0.1  # 0.25\n",
    "\n",
    "\n",
    "# copy training dataset images into subdirectories\n",
    "src_directory = './57398/ZooScanSet/Imgs/'  #'train/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create train, test & val folders\n",
    "# also, need to update this code for automatically reading large number of classes\n",
    "\n",
    "#for file in listdir(src_directory):\n",
    "for filename in list(trial.path):\n",
    "    #src = src_directory + '/' + file\n",
    "    # decide train or test\n",
    "    dst_dir = 'train/'\n",
    "    if random() < val_ratio:\n",
    "        dst_dir = 'test/'\n",
    "    \n",
    "    # apply image squaring & resizing\n",
    "    im = Image.open(str('./' + filename))\n",
    "    new_im = make_square(im)\n",
    "    imResize = im_resize(new_im)\n",
    "    \n",
    "    '''if file.startswith('cat'):\n",
    "\t\tdst = dataset_home + dst_dir + 'cats/'  + file\n",
    "\t\tcopyfile(src, dst)\n",
    "\telif file.startswith('dog'):\n",
    "\t\tdst = dataset_home + dst_dir + 'dogs/'  + file\n",
    "\t\tcopyfile(src, dst)'''\n",
    "    \n",
    "    if filename.find('Acantharea') != -1:\n",
    "        newname = filename.split(\"Acantharea\\\\\")[1]\n",
    "        dst = dataset_home + dst_dir + 'Acantharea/' + newname\n",
    "        imResize.save(dst)\n",
    "    elif filename.find('Copilia') != -1:\n",
    "        newname = filename.split(\"Copilia\\\\\")[1]\n",
    "        dst = dataset_home + dst_dir + 'Copilia/' + newname\n",
    "        imResize.save(dst)\n",
    "    elif filename.find('cypris') != -1:\n",
    "        newname = filename.split(\"cypris\\\\\")[1]\n",
    "        dst = dataset_home + dst_dir + 'cypris/' + newname\n",
    "        imResize.save(dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization of images?\n",
    "# /Desktop/data/theory/CNN_deepLearning_part4/sahana_talk/Kaggle_FER2013_Colab-Copy1_CNN_talk.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling outlier data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should not the no. of model parameters be less than the data available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network to use: built new or previously available? \n",
    "# what was used for crystal image data: \n",
    "#    Conf. paper (gray scale) : crystalNet (conv-pool * 3 -> conv -> FC -> FC -> output)\n",
    "#    google paper (color images): variant of Inception v3; (also tried Inception-ResNet-v2, NASNet)\n",
    "#    arxiv paper :\n",
    "# check applicability of existing CNN for gray-scale:\n",
    "# LeNet-5 : for gray scale  -> https://www.analyticsvidhya.com/blog/2018/12/guide-convolutional-neural-network-cnn/\n",
    "\n",
    "# custom CNN:\n",
    "# https://www.analyticsvidhya.com/blog/2019/01/build-image-classification-model-10-minutes/\n",
    "# https://towardsdatascience.com/a-simple-2d-cnn-for-mnist-digit-recognition-a998dbc1e79a\n",
    "\n",
    "# trying different hyper-parameters\n",
    "# https://www.kaggle.com/cdeotte/how-to-choose-cnn-architecture-mnist\n",
    "# https://www.kaggle.com/cdeotte/25-million-images-0-99757-mnist\n",
    "\n",
    "# use of transfer learning\n",
    "#https://nbviewer.jupyter.org/github/tirthajyoti/Deep-learning-with-Python/blob/master/Notebooks/Transfer_learning_CIFAR.ipynb\n",
    "\n",
    "# design of model & loss function suited to task  \n",
    "# https://www.kaggle.com/iafoss/pretrained-resnet34-with-rgby-0-460-public-lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "## optimise the model parameters & hyperparameters: kernel initialiser & padding *********\n",
    "\n",
    "import sys\n",
    "#from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten, Dropout\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cnn model\n",
    "'''def define_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    ## CHANGE INPUT SHAPE, BASED ON RGB OR GRAYSCALE\n",
    "    # specify padding or not?\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 1)))\n",
    "    \n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    \n",
    "    #model.add(Dense(1, activation='sigmoid'))  ## CHANGE HERE, BASED ON NO. OF CLASSES\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    # compile model\n",
    "    opt = SGD(lr=0.001, momentum=0.9)\n",
    "    #model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])  ## CHANGE HERE, BASED ON NO. OF CLASSES\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1229 23:57:15.500962 12960 deprecation_wrapper.py:119] From C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1229 23:57:15.570394 12960 deprecation_wrapper.py:119] From C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1229 23:57:15.583571 12960 deprecation_wrapper.py:119] From C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1229 23:57:15.626925 12960 deprecation_wrapper.py:119] From C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W1229 23:57:15.785025 12960 deprecation_wrapper.py:119] From C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1229 23:57:15.807016 12960 deprecation_wrapper.py:119] From C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "## CHANGE INPUT SHAPE, BASED ON RGB OR GRAYSCALE\n",
    "# specify padding or not?\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "\n",
    "#model.add(Dense(1, activation='sigmoid'))  ## CHANGE HERE, BASED ON NO. OF CLASSES\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# compile model\n",
    "opt = SGD(lr=0.001, momentum=0.9)\n",
    "#model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])  ## CHANGE HERE, BASED ON NO. OF CLASSES\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 200, 200, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 100, 100, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 100, 100, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 50, 50, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 80000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               10240128  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 10,333,187\n",
      "Trainable params: 10,333,187\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history):\n",
    "    # plot loss\n",
    "    plt.subplot(211)\n",
    "    plt.title('Cross Entropy Loss')\n",
    "    plt.plot(history.history['loss'], color='blue', label='train')\n",
    "    plt.plot(history.history['val_loss'], color='orange', label='test')\n",
    "    # plot accuracy\n",
    "    plt.subplot(212)\n",
    "    plt.title('Classification Accuracy')\n",
    "    plt.plot(history.history['acc'], color='blue', label='train')   # 'accuracy'\n",
    "    plt.plot(history.history['val_acc'], color='orange', label='test')\n",
    "    # save plot to file\n",
    "    #filename = sys.argv[0].split('/')[-1]\n",
    "    #pyplot.savefig(filename + '_plot.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## class_mode for multiclass?? ***** - check generator function\n",
    "# any other way of setting up? try without generator; see how to lead from csv or directly from images; sahana code\n",
    "#      classes will also need to be read in\n",
    "#      also, randomisation in the order is required\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "'''def run_test_harness():\n",
    "    \n",
    "    # define model\n",
    "    model = define_model()\n",
    "    \n",
    "    # create data generators\n",
    "    train_datagen = ImageDataGenerator(rescale=1.0/255.0,\n",
    "                                       width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "    test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    \n",
    "    ## prepare iterators       ## CHANGE class_mode, BASED ON NO. OF CLASSES\n",
    "    #train_it = train_datagen.flow_from_directory('dataset_dogs_vs_cats/train/',\n",
    "    #                                             class_mode='binary', batch_size=64, target_size=(200, 200))\n",
    "    #test_it = test_datagen.flow_from_directory('dataset_dogs_vs_cats/test/',\n",
    "    #                                           class_mode='binary', batch_size=64, target_size=(200, 200))    \n",
    "    train_it = train_datagen.flow_from_directory('./57398/ZooScanSet/runData/train/',\n",
    "                                                 class_mode='categorical', batch_size=64, target_size=(200, 200))\n",
    "    test_it = test_datagen.flow_from_directory('./57398/ZooScanSet/runData/test/',\n",
    "                                               class_mode='categorical', batch_size=64, target_size=(200, 200))\n",
    "    \n",
    "    # fit model\n",
    "    history = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n",
    "                                  validation_data=test_it, validation_steps=len(test_it), epochs=50, verbose=1)\n",
    "    # verbose can be 0,1,2; default is 1\n",
    "    # https://stackoverflow.com/questions/47902295/what-is-the-use-of-verbose-in-keras-while-validating-the-model\n",
    "    \n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate_generator(test_it, steps=len(test_it), verbose=1)\n",
    "    print('> %.3f' % (acc * 100.0))\n",
    "    \n",
    "    # learning curves\n",
    "    summarize_diagnostics(history)\n",
    "    \n",
    "run_test_harness()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2007 images belonging to 3 classes.\n",
      "Found 219 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "#model = define_model()\n",
    "\n",
    "\n",
    "# with data augmentation : how much is this augmenting?\n",
    "# create data generators\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0,\n",
    "                                   width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "## prepare iterators       ## CHANGE class_mode, BASED ON NO. OF CLASSES\n",
    "#train_it = train_datagen.flow_from_directory('dataset_dogs_vs_cats/train/',\n",
    "#                                             class_mode='binary', batch_size=64, target_size=(200, 200))\n",
    "#test_it = test_datagen.flow_from_directory('dataset_dogs_vs_cats/test/',\n",
    "#                                           class_mode='binary', batch_size=64, target_size=(200, 200))    \n",
    "train_it = train_datagen.flow_from_directory('./57398/ZooScanSet/runData/train/', color_mode='grayscale',\n",
    "                                             class_mode='categorical', batch_size=64, target_size=(200, 200))\n",
    "test_it = test_datagen.flow_from_directory('./57398/ZooScanSet/runData/test/', color_mode='grayscale',\n",
    "                                           class_mode='categorical', batch_size=64, target_size=(200, 200))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1229 23:58:13.974874 12960 deprecation.py:323] From C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1229 23:58:14.233107 12960 deprecation_wrapper.py:119] From C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "32/32 [==============================] - 199s 6s/step - loss: 10.2217 - acc: 0.3450 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 183s 6s/step - loss: 10.4774 - acc: 0.3500 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 191s 6s/step - loss: 10.4230 - acc: 0.3533 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 182s 6s/step - loss: 10.4910 - acc: 0.3491 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 168s 5s/step - loss: 10.5182 - acc: 0.3474 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 167s 5s/step - loss: 10.4774 - acc: 0.3500 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 165s 5s/step - loss: 10.5046 - acc: 0.3483 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 165s 5s/step - loss: 10.4910 - acc: 0.3491 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 194s 6s/step - loss: 10.4910 - acc: 0.3491 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 183s 6s/step - loss: 10.4774 - acc: 0.3500 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 125s 4s/step - loss: 10.4774 - acc: 0.3500 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 120s 4s/step - loss: 10.5590 - acc: 0.3449 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 130s 4s/step - loss: 10.4366 - acc: 0.3525 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 122s 4s/step - loss: 10.4910 - acc: 0.3491 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 115s 4s/step - loss: 10.4910 - acc: 0.3491 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 114s 4s/step - loss: 10.4910 - acc: 0.3491 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 116s 4s/step - loss: 10.4774 - acc: 0.3500 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 114s 4s/step - loss: 10.5046 - acc: 0.3483 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 127s 4s/step - loss: 10.5454 - acc: 0.3457 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 115s 4s/step - loss: 10.5182 - acc: 0.3474 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 113s 4s/step - loss: 10.5454 - acc: 0.3457 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 113s 4s/step - loss: 10.5046 - acc: 0.3483 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 124s 4s/step - loss: 10.4774 - acc: 0.3500 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 113s 4s/step - loss: 10.5454 - acc: 0.3457 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 111s 3s/step - loss: 10.5046 - acc: 0.3483 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 111s 3s/step - loss: 10.5046 - acc: 0.3483 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 111s 3s/step - loss: 10.5182 - acc: 0.3474 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 109s 3s/step - loss: 10.4774 - acc: 0.3500 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 121s 4s/step - loss: 10.5590 - acc: 0.3449 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 108s 3s/step - loss: 10.5182 - acc: 0.3474 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 113s 4s/step - loss: 10.5046 - acc: 0.3483 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 111s 3s/step - loss: 10.5046 - acc: 0.3483 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 111s 3s/step - loss: 10.5454 - acc: 0.3457 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 111s 3s/step - loss: 10.4910 - acc: 0.3491 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 111s 3s/step - loss: 10.5046 - acc: 0.3483 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 111s 3s/step - loss: 10.5182 - acc: 0.3474 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 111s 3s/step - loss: 10.4910 - acc: 0.3491 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 111s 3s/step - loss: 10.5318 - acc: 0.3466 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 119s 4s/step - loss: 10.4910 - acc: 0.3491 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 116s 4s/step - loss: 10.4910 - acc: 0.3491 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 111s 3s/step - loss: 10.5182 - acc: 0.3474 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 111s 3s/step - loss: 10.4774 - acc: 0.3500 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 108s 3s/step - loss: 10.5590 - acc: 0.3449 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 117s 4s/step - loss: 10.5318 - acc: 0.3466 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 108s 3s/step - loss: 10.4910 - acc: 0.3491 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 108s 3s/step - loss: 10.5046 - acc: 0.3483 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 108s 3s/step - loss: 10.5182 - acc: 0.3474 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 108s 3s/step - loss: 10.5454 - acc: 0.3457 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 112s 3s/step - loss: 10.5182 - acc: 0.3474 - val_loss: 11.4814 - val_acc: 0.2877\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 119s 4s/step - loss: 10.4774 - acc: 0.3500 - val_loss: 11.4814 - val_acc: 0.2877\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "history = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n",
    "                              validation_data=test_it, validation_steps=len(test_it), epochs=50, verbose=1)\n",
    "# verbose can be 0,1,2; default is 1\n",
    "# https://stackoverflow.com/questions/47902295/what-is-the-use-of-verbose-in-keras-while-validating-the-model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 3s 832ms/step\n",
      "> 30.137\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "_, acc = model.evaluate_generator(test_it, steps=len(test_it), verbose=1)\n",
    "print('> %.3f' % (acc * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': [11.481383436891042, 11.481383040615412, 11.481382883847031, 11.481382770625423, 11.481383445600397, 11.481383044970089, 11.481382892556386, 11.481383044970089, 11.481383162546376, 11.481383166901052, 11.481383166901052, 11.481383179965084, 11.481382901265741, 11.481383166901052, 11.48138276191607, 11.481382909975096, 11.481382766270746, 11.481382914329773, 11.481383441245718, 11.48138317125573, 11.481382901265741, 11.481382613857043, 11.481382774980101, 11.481382639985107, 11.481383162546376, 11.481383175610407, 11.481383044970089, 11.481383036260736, 11.481383175610407, 11.481383593659423, 11.481383158191699, 11.481382766270746, 11.481383166901052, 11.481383049324767, 11.48138317125573, 11.481383188674439, 11.481382883847031, 11.48138291868445, 11.48138317125573, 11.481383162546376, 11.481382901265741, 11.481382901265741, 11.481383166901052, 11.481382914329773, 11.481383049324767, 11.481382901265741, 11.481382639985107, 11.48138261821172, 11.481382766270746, 11.481383162546376], 'val_acc': [0.2876712330808378, 0.2876712330808378, 0.28767123219629404, 0.28767123314887966, 0.28767123314887966, 0.28767123219629404, 0.2876712341014653, 0.28767123219629404, 0.28767123219629404, 0.28767123314887966, 0.28767123314887966, 0.2876712323323777, 0.2876712323323777, 0.28767123314887966, 0.2876712330808378, 0.287671234237549, 0.28767123219629404, 0.287671233012796, 0.28767123219629404, 0.2876712341014653, 0.2876712323323777, 0.2876712341014653, 0.2876712341014653, 0.2876712321282522, 0.28767123219629404, 0.287671231379792, 0.28767123219629404, 0.2876712321282522, 0.287671231379792, 0.2876712321282522, 0.2876712330808378, 0.28767123219629404, 0.28767123314887966, 0.28767123314887966, 0.2876712341014653, 0.287671234237549, 0.28767123219629404, 0.2876712321282522, 0.2876712341014653, 0.28767123219629404, 0.2876712323323777, 0.2876712323323777, 0.28767123314887966, 0.287671233012796, 0.28767123314887966, 0.2876712323323777, 0.2876712321282522, 0.287671231379792, 0.28767123219629404, 0.28767123219629404], 'loss': [10.20206444430957, 10.504468723500494, 10.504468735379845, 10.504468737280542, 10.504468736805366, 10.50446884514505, 10.504468731578452, 10.504468646047124, 10.504468828513959, 10.50446884514505, 10.504468723500494, 10.50446875011024, 10.504468693802115, 10.504468706869401, 10.504468717798405, 10.504468676458263, 10.504468693089354, 10.504468761989592, 10.504468736330193, 10.504468797627645, 10.504468705919054, 10.504468651274038, 10.504468753911633, 10.504468645096775, 10.504468701167314, 10.504468670756173, 10.50446867598309, 10.504468693089354, 10.504468628465684, 10.504468706394228, 10.504468761989592, 10.504468701167314, 10.504468675507916, 10.504468626564988, 10.504468731578452, 10.504468736805366, 10.504468798102819, 10.504468761514417, 10.504468676458263, 10.504468646047124, 10.504468645571949, 10.504468662678216, 10.504468719699101, 10.504468670281, 10.504468798102819, 10.504468731578452, 10.50446867598309, 10.504468736330193, 10.504468797627645, 10.504468814733912], 'acc': [0.34578973577577315, 0.34828101627911034, 0.34828101614546764, 0.3482810163979039, 0.3482810162939595, 0.34828101627911034, 0.34828101651669735, 0.3482810163979039, 0.3482810163979039, 0.34828101627911034, 0.34828101627911034, 0.3482810164795744, 0.34828101626426117, 0.3482810163979039, 0.3482810163979039, 0.3482810163979039, 0.34828101627911034, 0.34828101651669735, 0.3482810163607809, 0.3482810162939595, 0.3482810163607809, 0.34828101651669735, 0.34828101627911034, 0.3482810163607809, 0.34828101651669735, 0.34828101651669735, 0.3482810162939595, 0.34828101627911034, 0.3482810164795744, 0.3482810162939595, 0.34828101651669735, 0.34828101651669735, 0.3482810163607809, 0.3482810163979039, 0.34828101651669735, 0.3482810162939595, 0.3482810163979039, 0.34828101641275305, 0.3482810163979039, 0.3482810163979039, 0.3482810162939595, 0.34828101627911034, 0.3482810164795744, 0.34828101641275305, 0.3482810163979039, 0.34828101651669735, 0.3482810162939595, 0.3482810163607809, 0.3482810162939595, 0.34828101627911034]}\n"
     ]
    }
   ],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d382c76160>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# learning curves\n",
    "#from matplotlib import pyplot\n",
    "#summarize_diagnostics(history)\n",
    "\n",
    "# plot loss\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Cross Entropy Loss')\n",
    "pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "# plot accuracy\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Classification Accuracy')\n",
    "pyplot.plot(history.history['acc'], color='blue', label='train')   # 'accuracy'\n",
    "pyplot.plot(history.history['val_acc'], color='orange', label='test')\n",
    "# save plot to file\n",
    "#filename = sys.argv[0].split('/')[-1]\n",
    "#pyplot.savefig(filename + '_plot.png')\n",
    "#pyplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model? -saving and loading a Keras model requires that the h5py library (HDF5 file format) is installed \n",
    "#!conda install h5py\n",
    "#!pip install h5py \n",
    "\n",
    "import h5py\n",
    "model.save('final_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### try different optimiser: Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1230 19:30:38.782941  8896 deprecation_wrapper.py:119] From C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1230 19:30:38.899724  8896 deprecation_wrapper.py:119] From C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1230 19:30:38.919617  8896 deprecation_wrapper.py:119] From C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1230 19:30:38.961873  8896 deprecation_wrapper.py:119] From C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W1230 19:30:39.126816  8896 deprecation_wrapper.py:119] From C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1230 19:30:39.170552  8896 deprecation_wrapper.py:119] From C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 200, 200, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 100, 100, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 100, 100, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 50, 50, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 80000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               10240128  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 10,333,187\n",
      "Trainable params: 10,333,187\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "## CHANGE INPUT SHAPE, BASED ON RGB OR GRAYSCALE\n",
    "# specify padding or not?\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "\n",
    "#model.add(Dense(1, activation='sigmoid'))  ## CHANGE HERE, BASED ON NO. OF CLASSES\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# compile model\n",
    "#opt = SGD(lr=0.001, momentum=0.9)\n",
    "#model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])  ## CHANGE HERE, BASED ON NO. OF CLASSES\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2007 images belonging to 3 classes.\n",
      "Found 219 images belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1230 19:33:31.857776  8896 deprecation.py:323] From C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1230 19:33:32.193625  8896 deprecation_wrapper.py:119] From C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "32/32 [==============================] - 207s 6s/step - loss: 10.7346 - acc: 0.3174 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 198s 6s/step - loss: 11.0738 - acc: 0.3130 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 194s 6s/step - loss: 11.1146 - acc: 0.3104 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 173s 5s/step - loss: 11.0874 - acc: 0.3121 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 194s 6s/step - loss: 11.0874 - acc: 0.3121 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 208s 6s/step - loss: 11.0738 - acc: 0.3130 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 177s 6s/step - loss: 11.1282 - acc: 0.3096 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 128s 4s/step - loss: 11.1146 - acc: 0.3104 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 133s 4s/step - loss: 11.0874 - acc: 0.3121 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 116s 4s/step - loss: 11.0602 - acc: 0.3138 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 134s 4s/step - loss: 11.0874 - acc: 0.3121 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 145s 5s/step - loss: 11.1146 - acc: 0.3104 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 136s 4s/step - loss: 11.1282 - acc: 0.3096 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 127s 4s/step - loss: 11.1010 - acc: 0.3113 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 138s 4s/step - loss: 11.1146 - acc: 0.3104 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 154s 5s/step - loss: 11.1282 - acc: 0.3096 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 119s 4s/step - loss: 11.1146 - acc: 0.3104 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 118s 4s/step - loss: 11.1282 - acc: 0.3096 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 130s 4s/step - loss: 11.1282 - acc: 0.3096 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 124s 4s/step - loss: 11.0738 - acc: 0.3130 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 124s 4s/step - loss: 11.0602 - acc: 0.3138 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 127s 4s/step - loss: 11.0874 - acc: 0.3121 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 118s 4s/step - loss: 11.1010 - acc: 0.3113 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 115s 4s/step - loss: 11.1010 - acc: 0.3113 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 127s 4s/step - loss: 11.1282 - acc: 0.3096 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 129s 4s/step - loss: 11.1554 - acc: 0.3079 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 139s 4s/step - loss: 11.0057 - acc: 0.3172 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 120s 4s/step - loss: 11.1282 - acc: 0.3096 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 117s 4s/step - loss: 11.1010 - acc: 0.3113 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 117s 4s/step - loss: 11.0466 - acc: 0.3146 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 118s 4s/step - loss: 11.1554 - acc: 0.3079 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 116s 4s/step - loss: 11.1146 - acc: 0.3104 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 118s 4s/step - loss: 11.0874 - acc: 0.3121 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 118s 4s/step - loss: 11.1418 - acc: 0.3087 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 125s 4s/step - loss: 11.1282 - acc: 0.3096 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 115s 4s/step - loss: 11.0193 - acc: 0.3163 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 129s 4s/step - loss: 11.1418 - acc: 0.3087 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 115s 4s/step - loss: 11.0738 - acc: 0.3130 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 119s 4s/step - loss: 11.1282 - acc: 0.3096 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 123s 4s/step - loss: 11.1554 - acc: 0.3079 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 114s 4s/step - loss: 11.1826 - acc: 0.3062 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 114s 4s/step - loss: 11.1146 - acc: 0.3104 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 116s 4s/step - loss: 11.0874 - acc: 0.3121 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 115s 4s/step - loss: 11.1010 - acc: 0.3113 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 126s 4s/step - loss: 11.0738 - acc: 0.3130 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 129s 4s/step - loss: 11.1690 - acc: 0.3071 - val_loss: 10.4510 - val_acc: 0.3516\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 138s 4s/step - loss: 11.1146 - acc: 0.3104 - val_loss: 10.4510 - val_acc: 0.3516\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0,\n",
    "                                   width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "## prepare iterators       ## CHANGE class_mode, BASED ON NO. OF CLASSES\n",
    "#train_it = train_datagen.flow_from_directory('dataset_dogs_vs_cats/train/',\n",
    "#                                             class_mode='binary', batch_size=64, target_size=(200, 200))\n",
    "#test_it = test_datagen.flow_from_directory('dataset_dogs_vs_cats/test/',\n",
    "#                                           class_mode='binary', batch_size=64, target_size=(200, 200))    \n",
    "train_it = train_datagen.flow_from_directory('./57398/ZooScanSet/runData/train/', color_mode='grayscale',\n",
    "                                             class_mode='categorical', batch_size=64, target_size=(200, 200))\n",
    "test_it = test_datagen.flow_from_directory('./57398/ZooScanSet/runData/test/', color_mode='grayscale',\n",
    "                                           class_mode='categorical', batch_size=64, target_size=(200, 200))\n",
    "\n",
    "# fit model\n",
    "history = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n",
    "                              validation_data=test_it, validation_steps=len(test_it), epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 1s/step\n",
      "> 33.790\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "_, acc = model.evaluate_generator(test_it, steps=len(test_it), verbose=1)\n",
    "print('> %.3f' % (acc * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(len(train_it))  # this is the no. of iterations per epoch = train_data_size/ batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 4s 988ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[10.156608124301858, 0.3698630139027556]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(test_it, steps=len(test_it), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### round 2 : try different model ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0103 16:38:27.161437  8896 deprecation.py:506] From C:\\Users\\DAR9KOR\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 200, 200, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 100, 100, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 100, 100, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 50, 50, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 50, 50, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 40000)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                2560064   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 2,588,323\n",
      "Trainable params: 2,588,323\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "## CHANGE INPUT SHAPE, BASED ON RGB OR GRAYSCALE\n",
    "# specify padding or not?\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#model.add(Dense(1, activation='sigmoid'))  ## CHANGE HERE, BASED ON NO. OF CLASSES\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# compile model\n",
    "opt = SGD(lr=0.001, momentum=0.9)\n",
    "#model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])  ## CHANGE HERE, BASED ON NO. OF CLASSES\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2007 images belonging to 3 classes.\n",
      "Found 219 images belonging to 3 classes.\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - 93s 3s/step - loss: 1.2386 - acc: 0.3571 - val_loss: 1.0484 - val_acc: 0.4977\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 105s 3s/step - loss: 1.0040 - acc: 0.4406 - val_loss: 0.8397 - val_acc: 0.6530\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 114s 4s/step - loss: 0.8646 - acc: 0.5088 - val_loss: 0.6558 - val_acc: 0.6027\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 122s 4s/step - loss: 0.8207 - acc: 0.5459 - val_loss: 0.7637 - val_acc: 0.6073\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 165s 5s/step - loss: 0.8409 - acc: 0.5278 - val_loss: 0.6949 - val_acc: 0.5616\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 143s 4s/step - loss: 0.8533 - acc: 0.5361 - val_loss: 0.7087 - val_acc: 0.5753\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 125s 4s/step - loss: 0.8593 - acc: 0.5199 - val_loss: 0.7843 - val_acc: 0.6164\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 148s 5s/step - loss: 0.8330 - acc: 0.5238 - val_loss: 0.7171 - val_acc: 0.6575\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 143s 4s/step - loss: 0.8811 - acc: 0.5120 - val_loss: 0.6446 - val_acc: 0.6073\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 152s 5s/step - loss: 0.8085 - acc: 0.5372 - val_loss: 0.6210 - val_acc: 0.6164\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 171s 5s/step - loss: 0.8368 - acc: 0.5072 - val_loss: 0.6233 - val_acc: 0.6667\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 155s 5s/step - loss: 0.8072 - acc: 0.5346 - val_loss: 0.6484 - val_acc: 0.6073\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 150s 5s/step - loss: 0.8247 - acc: 0.5259 - val_loss: 0.6969 - val_acc: 0.6164\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 128s 4s/step - loss: 0.8325 - acc: 0.5327 - val_loss: 0.6325 - val_acc: 0.6621\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 141s 4s/step - loss: 0.8148 - acc: 0.5435 - val_loss: 0.6643 - val_acc: 0.6073\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 150s 5s/step - loss: 0.8179 - acc: 0.5203 - val_loss: 0.6300 - val_acc: 0.5982\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 130s 4s/step - loss: 0.8135 - acc: 0.5286 - val_loss: 0.6233 - val_acc: 0.5982\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 127s 4s/step - loss: 0.8017 - acc: 0.5550 - val_loss: 0.6301 - val_acc: 0.6119\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 128s 4s/step - loss: 0.7995 - acc: 0.5404 - val_loss: 0.6219 - val_acc: 0.6484\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 131s 4s/step - loss: 0.8137 - acc: 0.5317 - val_loss: 0.6692 - val_acc: 0.6164\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 150s 5s/step - loss: 0.7951 - acc: 0.5457 - val_loss: 0.6488 - val_acc: 0.6119\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 155s 5s/step - loss: 0.7984 - acc: 0.5522 - val_loss: 0.5962 - val_acc: 0.6164\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 156s 5s/step - loss: 0.8212 - acc: 0.5358 - val_loss: 0.6355 - val_acc: 0.6119\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 119s 4s/step - loss: 0.8140 - acc: 0.5361 - val_loss: 0.6020 - val_acc: 0.6119\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 120s 4s/step - loss: 0.7878 - acc: 0.5534 - val_loss: 0.6202 - val_acc: 0.6073\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 156s 5s/step - loss: 0.8099 - acc: 0.5394 - val_loss: 0.6182 - val_acc: 0.6210\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 125s 4s/step - loss: 0.8363 - acc: 0.5273 - val_loss: 0.6239 - val_acc: 0.6484\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 160s 5s/step - loss: 0.8017 - acc: 0.5443 - val_loss: 0.6444 - val_acc: 0.6164\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 136s 4s/step - loss: 0.8011 - acc: 0.5384 - val_loss: 0.6818 - val_acc: 0.5982\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 161s 5s/step - loss: 0.8223 - acc: 0.5632 - val_loss: 0.7085 - val_acc: 0.6119\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 129s 4s/step - loss: 0.8036 - acc: 0.5463 - val_loss: 0.6127 - val_acc: 0.6210\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 138s 4s/step - loss: 0.8082 - acc: 0.5313 - val_loss: 0.6109 - val_acc: 0.6164\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 155s 5s/step - loss: 0.8022 - acc: 0.5403 - val_loss: 0.6027 - val_acc: 0.6484\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 156s 5s/step - loss: 0.7873 - acc: 0.5512 - val_loss: 0.6354 - val_acc: 0.6256\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 137s 4s/step - loss: 0.7933 - acc: 0.5486 - val_loss: 0.6296 - val_acc: 0.6119\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 127s 4s/step - loss: 0.7952 - acc: 0.5624 - val_loss: 0.6305 - val_acc: 0.6119\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 128s 4s/step - loss: 0.7911 - acc: 0.5435 - val_loss: 0.6492 - val_acc: 0.6164\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 128s 4s/step - loss: 0.8083 - acc: 0.5391 - val_loss: 0.6130 - val_acc: 0.6164\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 130s 4s/step - loss: 0.7925 - acc: 0.5446 - val_loss: 0.6133 - val_acc: 0.6119\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 149s 5s/step - loss: 0.7615 - acc: 0.5655 - val_loss: 0.5956 - val_acc: 0.6119\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 145s 5s/step - loss: 0.7970 - acc: 0.5397 - val_loss: 0.6026 - val_acc: 0.6484\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 145s 5s/step - loss: 0.7946 - acc: 0.5380 - val_loss: 0.6220 - val_acc: 0.6210\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 128s 4s/step - loss: 0.7774 - acc: 0.5569 - val_loss: 0.6249 - val_acc: 0.6347\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 126s 4s/step - loss: 0.7786 - acc: 0.5523 - val_loss: 0.6002 - val_acc: 0.6301\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 140s 4s/step - loss: 0.7851 - acc: 0.5605 - val_loss: 0.6170 - val_acc: 0.6256\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 138s 4s/step - loss: 0.7835 - acc: 0.5532 - val_loss: 0.6042 - val_acc: 0.6256\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 136s 4s/step - loss: 0.7640 - acc: 0.5566 - val_loss: 0.5833 - val_acc: 0.6256\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 88s 3s/step - loss: 0.7820 - acc: 0.5505 - val_loss: 0.6325 - val_acc: 0.6438\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 89s 3s/step - loss: 0.7895 - acc: 0.5433 - val_loss: 0.6026 - val_acc: 0.6530\n"
     ]
    }
   ],
   "source": [
    "# create data generators\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0,\n",
    "                                   width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "## prepare iterators       ## CHANGE class_mode, BASED ON NO. OF CLASSES\n",
    "#train_it = train_datagen.flow_from_directory('dataset_dogs_vs_cats/train/',\n",
    "#                                             class_mode='binary', batch_size=64, target_size=(200, 200))\n",
    "#test_it = test_datagen.flow_from_directory('dataset_dogs_vs_cats/test/',\n",
    "#                                           class_mode='binary', batch_size=64, target_size=(200, 200))    \n",
    "train_it = train_datagen.flow_from_directory('./57398/ZooScanSet/runData/train/', color_mode='grayscale',\n",
    "                                             class_mode='categorical', batch_size=64, target_size=(200, 200))\n",
    "test_it = test_datagen.flow_from_directory('./57398/ZooScanSet/runData/test/', color_mode='grayscale',\n",
    "                                           class_mode='categorical', batch_size=64, target_size=(200, 200))\n",
    "\n",
    "\n",
    "# fit model\n",
    "history = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n",
    "                              validation_data=test_it, validation_steps=len(test_it), epochs=50, verbose=1)\n",
    "# started ~ 4:39pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-01-03 18:31:42'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "datetime.now().strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d2854bf208>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOyddXhUR9fAfxMlSAgkeAjBixR3akCFugt1f6vfW3eh7u7ytlSoC6UtdQqU4q4FgjtEiEBCZM/3x7nbbJLdzSbZZEmY3/PcZ3fvnTtz5u69Z86cOTPXiAgWi8Viqf2EhVoAi8VisQQHq9AtFouljmAVusVisdQRrEK3WCyWOoJV6BaLxVJHsArdYrFY6ghWoVssFksdwSp0S6UwxpxvjJlnjMkxxmw3xvxkjDkshPKMM8bkO/K4t8UBnjvWGPNxdcsYKMaYDcaYo0Mth6X2YRW6pcIYY24BXgQeB1oAScDrwKk+0kfUkGhPi0hDj613MDI1in1WLAc89ia1VAhjTGPgYeB6EflGRPaKSIGIfC8itztpxhpjvjLGfGyMyQIuNcZEG2NeNMZsc7YXjTHRTvoEY8wPxpg9xph0Y8xfbgVqjLnTGLPVGJNtjFlljBlVCZmTjTFijLnEGLPJGJNqjLnXOTYauAc419OqN8ZMMcY8Zoz5G9gHdDDGtDbGTHRkTDHGXOVRhrvOnzuyLjDG9HaO3W6M+bqUTK8YY16sRF2ucspOd2Rp7ew3xpgXjDG7jDGZxpglxpiezrETjDErHLm2GmNuq2i5llqCiNjNbgFvwGigEIjwk2YsUACchhoNMWgjMAtoDjQDZgCPOOmfAN4EIp3tcMAAXYHNQGsnXTLQ0UeZ44BHfRxLBgR4x5GlN7Af6OYh78elzpkCbAJ6ABGOXFPRnkg9oA+wGxhVqs5nOWlvA9Y731sBe4E4J20EsAvo70PeDcDRXvaPBFKBfkA08AowzTl2HDAfiHOuXTeglXNsO3C4870J0C/U95HdqmezFrqlosQDqSJSWE66mSIyQURcIpILXAA8LCK7RGQ38BBwkZO2AFV67USt/b9ERIAiVHF1N8ZEisgGEVnrp8zbHCvfvX1Q6vhDIpIrIouBxahi98c4EVnu1LUlcBhwp4jkicgi4F2POgDMF5GvRKQAeB5V/ENEZDswDTjbSTcavYbzyym/NBcA74nIAhHZD9wNDDXGJKPXsBFwCGBEZKVTLs6x7saYWBHJEJEFFSzXUkuwCt1SUdKAhAD84ptL/W4NbPT4vdHZB/AMkAL8aoxZZ4y5C0BEUoCbUOt3lzHmM7eLwQfPikicx3ZJqeM7PL7vAxpWoA6tgXQRyS5Vhzbe0ouIC9jiUccPgAud7xcCH5VTtjdKXEMRyUH/jzYiMhl4FXgN2GmMedsYE+skPRM4AdhojJlqjBlaibIttQCr0C0VZSaQh7pT/FF6Gc9tQDuP30nOPkQkW0RuFZEOwMnALW5fuYh8IiKHOecK8FTVq1CurN72bwOaGmMaeexLArZ6/G7r/uKMASQ65wFMAHo5fu2TgPGVkLPENTTGNEB7TFsBRORlEemPuom6ALc7++eKyKmou2sC8EUlyrbUAqxCt1QIEckEHgBeM8acZoypb4yJNMYcb4x52s+pnwL3GWOaGWMSnDw+BjDGnGSM6WSMMUAW6mopMsZ0NcaMdAZP84Bc51iw2Qkk+4tkEZHNqN//CWNMPWNML+AKSirm/saYM5zey02on36Wc34e8BXwCTBHRDaVI1OkU457i3DOvcwY08e5Jo8Ds0VkgzFmoDFmsDEmEvXX56HXMMoYc4ExprHjCnJfX0sdxCp0S4URkeeBW4D70IHBzcANqPXni0eBecASYCmwwNkH0Bn4HchBewCvi8gU1H/+JDoQuAO1MO/xU8YdpmQcemqAVfrS+UwzxvjzL49BB1i3Ad8CD4rIbx7HvwPOBTJQ3/oZjhJ18wFwKIG5WyahDZh7GysifwD3A1+jA50dgfOc9LHooG8G6pZJA551jl0EbHAijq6h2PVjqWMYHXuyWCxVwRgzFugkIj6VpTEmCfgHaCkiWTUlm+XgwVroFksN4LhzbgE+s8rcUl3U1Aw+i+WgxRm83Im6QkaHWBxLHca6XCwWi6WOYF0uFovFUkcImcslISFBkpOTQ1W8xWKx1Ermz5+fKiLNvB0LmUJPTk5m3rx5oSreYrFYaiXGmI2+jlmXi8VisdQRaqVCF9HNYrFYLMXUOoU+YQLEx8OWLaGWxGKxWA4sap1Cb94cMjJg4cJQS2KxWCwHFrVOoffqBcZYhW6xWCylKVehG2Pec15rtczH8Quc110tMcbMcL92q7po2BC6drUK3WKxWEoTiIU+Dv/TldcDR4pIL+AR4O0gyOWXvn1hgX3nisVisZSgXIUuItOAdD/HZ4hIhvNzFrqof7XSty9s3gxpadVdksVisdQegu1DvwL4Kch5lqFvX/20bheLxWIpJmgK3RgzAlXod/pJc7UxZp4xZt7u3bsrXZZV6BaLxVKWoCh053Vc7wKniohPR4iIvC0iA0RkQLNmXpciCIj4eEhKsgrdYrFYPKmyQnfewvINcJGIrK66SIHRt69V6BaLxeJJuYtzGWM+BY4CEowxW4AHgUgAEXkTfdlvPPC6vuOXQhEZUF0Cu+nbFyZOhJwcDWW0WCyWg51yFbqIjCnn+JXAlUGTKED69tX1XJYsgWHDarp0i8ViOfCodTNF3diBUYvFYilJrVXoiYmQkGAVusVisbiptQrdGDswarFYLJ7UWoUOqtCXLoX8/FBLYrFYLKGn1iv0ggJYsSLUklgsFkvoqfUKHazbxWKxWKCWK/TOnTUG3Sp0i8ViqeUKPSwMeve2Ct1isViglit0ULfLokXgcoVaEovFYgktdUKh5+RASkqoJbFYLJbQUusVer9++mndLhaL5WCn1iv07t0hMtIqdIvFYqn1Cj0qCnr2tArdYrFYar1Ch+IlAERCLYnFYrGEjtqn0LNWw9KHoTD33119+8Lu3bB1awjlslgslhBTCxX6P7D0QchY8O8uOzBqsVgstVGhxw/Wz9RZ/+7q1UtXX7QK3WKxHMzUPoUe0wIatC+h0Bs2hC5drEK3WCwHN7VPoQMkDIa02SV22bXRLRbLwU65Ct0Y854xZpcxZpmP48YY87IxJsUYs8QY0y/4YpYifgjs2wz7ikdB+/aFjRshLa3aS7dYLJYDkkAs9HHAaD/Hjwc6O9vVwBtVF6scEobop4eV7h4YXbSo2ku3WCyWA5JyFbqITAPS/SQ5FfhQlFlAnDGmVbAE9EqTPhAWVcKP3r8/hIfD779Xa8kWi8VywBIMH3obYLPH7y3OvjIYY642xswzxszbvXt35UsMj4Ym/Uoo9CZN4Oij4fPP7QQji8VycBIMhW687POqUkXkbREZICIDmjVrVrVSE4ZA+jxwFfy769xzYf16mDevallbLBZLbSQYCn0L0NbjdyKwLQj5+id+MBTlwp6l/+467TRdqOvzz6u9dIvFYjngCIZCnwhc7ES7DAEyRWR7EPL1j5eB0SZN4Ljj4Isv7AsvLBbLwUcgYYufAjOBrsaYLcaYK4wx1xhjrnGSTALWASnAO8B11SatJw3aQb0WJfzooG6XzZth1iwf51ksFksdJaK8BCIyppzjAlwfNIkCxRi10ksp9FNOgehodbsMG1bjUlksFkvIqJ0zRd3ED4Hs1bC/eDZRbCyccAJ8+SUUFYVQNovFYqlhardC/9ePPqfE7nPPhe3bYfr0EMhksVgsIaJ2K/SmA8CElXG7nHQS1K9vo10sFsvBRe1W6JENoXHPMgq9QQNV6l99BYWFIZLNYrFYapjardBB3S5pc0BKximee66+xWjKlNCIZbFYLDVN7Vfo8UOgYI++ms6D44/XddI/+yxEclksFksNU/sV+r8DoyXdLjExcOqp8M03kJ8fArksFoulhqn9Cj22K0Q2LuNHB3W7ZGTYFRgtFsvBQe1X6CZM13XxotCPPRYaN7bRLhaL5eCg9it0ULdL5lIoyCmxOzoazjgDJkyAvLwQyWaxWCw1RN1Q6PGDNcolvey6ueeeC1lZ8MsvIZDLD1OnalilxWKxBIu6odATButnqRdHA4wcCc2bw333QWZmDcvlBRF46SWV65xzbFilxWIJHnVDoUfHQ6POXv3okZHw8cfwzz9w5pnBjXjJytJ3mH7zDbz8MqxY4T99QQFcdx3cdJMuIta5M1x0kQ7cWiwWS1Upd7XFWkP8ENjxm5rApuRLlI45Bt59Fy69FK68Ej74oEwSv7hcsHy5WtOzZkFKCqxdC2lpJdMZA+edBw88AIccUvJYRgacfTb88QfcdRc89hgsWABDh6qS/+STislksVgspak7Cj1hCGz4CPZt0rXSS3HJJbpO+v33Q1ISPPqo76xEYPFiVeBTp8K0aZDuvCY7MRG6dYOzzoIOHaBjR/1s2hTefBNeeUWjas4/X8vq0gXWrNGlCNavh/ff14YFYMAAGDtW3UEnnggXXhjsi2KxWA4qRCQkW//+/SWopM0XGY/I+k99JnG5RK68UgRE3nqr7PH9+0XGjRPp0UPTgEj79iKXXab7168vX4xdu0Ruv12kfn2RsDCRc88VadJEJD5eZNq0sukLC0WGDxeJjfWf/7ffarrPPitfBkv1snmzyMkni1xzjUhubqilsRxsAPPEh16tOwq9qEDk84Yic671m6ygQOSEE1TZfv+97svKEnnuOZHERL0iPXuKvPOOyMaNlRdnxw6RW24RqVdPpFs3kZQU32nXrRNp1Ejk8MNVwXuya5c2CiDSsKF+XnCBSEZG+TLk52ve06eLfPmlyEsvidx5p8hFF4lcfrnIV1+JZGdXvo4HI19/rQ10vXr6XwweLLJ1a9XyTE1VY8NiCYSDQ6GLiEweLfJD93KTZWeL9O+vVvSNN4o0bqxX4qijRCZNCu7DlZ4emBX34Ycqw2OP6W+XS63xhASRyEiRRx4R2bdP5KGHRMLDRdq2FZk82Xte69ap4m7WrLin4d4iI0XatVOlBCLR0SInnijy9tsi27eXzauwUGTPHlVa+/dX+jLUKIsXi3zxhcisWVqnoqKq55mTI3LVVXrNBgwQWb1a5JtvRBo0EGnZUmTGjIrnuWSJXnvQ+/Gbb4Ijq6VuU2WFDowGVqHvDb3Ly/Ek4E9gIbAEOKG8PKtFoS9/Ut0uuTvLTbp9u0hysogxImeeqQ9/KHG51BKPiNCew+mn678zcKDI0qUl086eLdK5sx6/5RZtMAoLRSZOFDn+eK1TeLjm8e67Ij//rMpj9+7ixqqgQBuE//5XrwPoeT16iBxyiEjr1sU9AvdmjEirVmqVnn22yG23ibz8ssjMmWV7FoGyb5/I55+rC6NlS5GbbxbZsqVy12/yZJFjjy3biEVHi3TqJDJqlMhNN4nsLP/2KMH8+SJdu2r977qrZMO2dKlIhw4iUVF6rQNh0yaRSy/V/Bo31jp36qSy9ughMn68/j8Wizf8KXSjx31jjAkHVgPHAFuAucAYEVnhkeZtYKGIvGGM6Q5MEpFkf/kOGDBA5s0rOxGoSqTOgl+HwmFfQtJZ5SdPhb17oV3ZMdSQkJEBvXrBli06y/Xhh+GWWyDCy9D13r1wxx3w+usaUbNvH2zaBK1bw1VX6damTWDlisDSpfDddzB7tr4cJDa25BYTA7t2aRmem3sGbtOmGk00ejQcdxy0auW7vKIimDwZxo/XkM/sbJW1b1/46ScIC9OB4zvugE6d/MvuculM4CefhLlzoUULDQs97ji9jps2wcaNxZ/z5+t6+U88AVdfrWX5IicHXntNB7ebN4ePPoIRI8qmS0/X6KbffoPrr4cXXtBw2dJkZKicL7+sct94I9xzj167wkJ9beJjj2lEVceOGg3Vo4den6ws3dzfw8P1vPh43dzfmzfXe6cmKSzUt4OtXQuDBqnM/q6rpWoYY+aLyACvB31pevcGDAV+8fh9N3B3qTRvAXd6pJ9RXr7VYqEX5Yt83kBk7g3Bz7uGmDlT5PzzRVauDCz9pElqUR99tPp38/OrVz5PXC6RbdvUNXTppWphu63i3r1FzjtPez+nnKI9h1GjRI44ojhdbKz68idPLrbw160Tue46tarDwjSPhQt1TGLFCpG//hKZMEHkvfdEHn9cLWcQ6dhR5M03y3dvrVwpMmKEnjNokMiCBWXT/POPyP/9n8oH2tNJTfWfb0GByK23avpWrVSubt3U4u7VS6RPH5G4OLXKL7pIZMMG7/kUFekA+IABZXsagWwxMXrNP/usesdHcnO1J3n55Trg7ylDXJz+3489JjJlivbCLMGDKlroZwGjReRK5/dFwGARucEjTSvgV6AJ0AA4WkTme8nrauBqgKSkpP4bN24MtFEKnMnHQe42OHFp8PO2+EUEliyBn3/WbcsWiIpSa9X9GRmpVvQ552ioZr163vPasUMt3ddfV0vZF337qiV75plqtQYq5yefaO8nNRVuuEHDR6dOVYv8999VzrPOUot72LDA5wh8+SV8+61a4C6X9kbcn40bw623Qp8+gck4Y4Za5I0aFfeUGjXSrahIewbp6Tofwv25cCF8/TXs3KnX9vjjdf7DqFGwdSusWlVyW7tWLezwcLWq3Vt4uL5PoGlTaNJEP93f16yBSZP0f4mN1ZDc00+Hnj1hzhz4+2+12N0T7WJi4Kmn9DqXdx0zMrRHtHEjxMUVb02a6Gf79jp3w9d9cyBTWKi9rzlzoHt3GD68cvn4s9ADUehnA8eVUuiDRORGjzS3OHk9Z4wZCvwP6ClS6jVCHlSLywVg+eOw+F44YzfUSwh+/t4QgW2TIGEoRDetmTIPEjIy4NNP9RJ7uhbc32Njq5b3vffq/AFjVPG2aQPXXKMT0Fq2DF49apKiIlWqX36pyn379pLHjdG5GF276mzl6OjiBsjd+BQVqcJ2Nxrp6Xq9MjKgWTN918AZZ6gLKirKuxzp6doovfGGNgAnnqjzMJo1855+0iS97rt3a+OQmanlZWbq/++mXj04/HB18R19NPTuXb6LR0Tz2bxZty1bivPy3GJi1GXUtIqPcV6eNqDz56sCnz1bv+fm6vGbblKDpTJUVaEPBcaKyHHO77sBROQJjzTLUSt+s/N7HTBERHb5yrfaFPruGfDbcDj8a2h7RvDz98bmCfDX6dDyGBjxi53yWcuYMwfGjVPlcMop3scsaisulyrVWbN0rMitxGNiKp+fMRW7xUW053PbbWppf/CBLm3tJitLe0v/+58q0w8/hH79SpaZna3Kffly7UH99pt+B0hIgMGD9X9zuYqdPy6XLrexbZsq8ezswOStX1/HV269VScS+qKoSGX55RdtNHfsKN727ClOFx2t9Rk0SOUcNEgnI1ZWTVRVoUegg6KjgK3ooOj5IrLcI81PwOciMs4Y0w34A2gjfjKvNoVelA9fxUHHq2DAS8HPvzSF++CHbvoavIIsGPgGdL6m+su1WGoZS5fCmDGqiG+9VQeAp0+Hyy9Xi/mOO9T1Feig7vbtqlB//11ndkNxYxMWpp/h4Roo0Lat9krattUtMVGP5eWp1ZyXp1t2tvYIx4/3PTi/dKk2OuPHqwwxMdqza9myeGvRQgMD+vSBQw/13YupDFUaFHV08gmoUl8L3Ovsexg4xfneHfgbWAwsAo4tL89qGRR188fRIj/2Kj/d9t/8ziwNiEX3aqjkzqla7ucNRLLXVi1Pi6WOsm+fyPXXqw2dlKSfXbpoMMCBxPr1ItdeWzw4P2aMyDPP6OA2aHjxqadqIEJeXs3KRlUGRauLarPQAZY9CksegDNTffu0i/Lhu3awfxcc83fxu0krQtZqmHQoJJ0Lwz6EvZv0d5M+MOpPfZuSxWIpw8SJ6kc+9VS11OvXD7VE3tm+XX3db7yhYwoDBui6UOee63ssoLqpksuluqhWhb7rL/j9CDj8W2h7mvc0Gz6DGWMgogHUbwvHL4TwCgydi8Cfo/Xl1CetghhnBG3t+zD7cuj3AhxyU9XrYrFYQs6ePerDb98+1JL4V+h104SMH6TKeddU32nWvAoNO8FhX0HWP7B0bMXK2PwN7PgVej1arMwBOlwKrU+CxXdD1qrKSG+xWA4w3CGTBzp1U6GHR2sIoS+Fnr4Qdv8NXa6H1qOh4xWw8hlImxtY/oV7YcFNENcbOl9b8pgxMPhtCI+BmZeAq7BqdbFYLJYAqZsKHaD5UZCxCPK9vA5o9asQXl+taYC+z0FMa5h1KRTtLz/vZY/Avi0w8DUI8xLjFtMKBryur8Rb+WwVKmGxWCyBU4cV+pGAwK7pJffvT4ONn0D7iyAqTvdFNYZBb0PmClj2sP98M1fCyue0MWjmZ6pXu3Oh7Vmw9AHYY2etWiyW6qfuKvSEwRAWDbumlNy/9j0oylN3iyetj4cOl8GKpyC9zKoFigjMu1EHUvs85b98Y2Dg6xDVBGZdDq6iSlfFUscJpFdosQRA3VXo4fU0FNHTj+4qgjWvqzsm7tCy5/R7Huq1gJmXalgj6MO27ReYewN8lww7/4Dej0G95uXLUK8Z9HsJ0uepm8di8SR3J8y6Ar5oAFsnhVoaSx2g7ip0ULdLxkLIz9Tf2ybB3g3Q5Qbv6aPiHNfLMph1Cfx1FnydAFNGw7r3NL58yAdlB0L90e5caDUaltwHezdXuUoVYuskHfy1HFgU5cPK5+GHLvoe3MhYjbIKUQixpe5QtxV6i6NAXLDb8aOvfhXqJ0Liqb7PaXMitL8ENn4GqTMh+Xw48gc4Mw2O/A46XFyxCUNu14sUwbwbau6h3fAJTD0J/jwectbXTJmW8tn+K/zUGxbeCgnD4YRl0OdJSJ+rvT+LpQrUbYUePwTCotTtkrVK48Y7XeM9MsWTQW/DicvhtC0w6C1V8hGVXM0IoGF7OPQh2DoRtkyofD6BsuV7mHkxNBumjc+MC234ZKjZtxWmnQZ/Hqf/xZE/wIhJENtFDYiYVrpSqMVSBeq2Qo+IgfjBsHMKrH5NlXunq8o/LzwKGncP7qqJhzhx6/Nu1EW8qoudf8L0s6FJPzjqJ+0dpM6A5U+Uf66lekidBT8PgB2/62D6icvUSHATHg2H3Kb/Xeqs0MlpqfXUbYUOjh99AawbB0nnBDaYWR2ERarln7tN12uvDlJnw9RToFEnGPETRDZSl1G782HZQ3q8PFwF1SNbeWQs0jGLDZ/Vrd7E+o/g96Mgoj4cOxu636EKvDSdroaoprbhtVSJuq/QWxyl/uvCbN+DoTVFwiANl1z9WmDKtSLsWQpTjtcGa8SvEB1ffGzgaxDTRl0vBT5e/5OfCTMuhi8awaqXa3aAbs9ymHwMbPlW19eZ2FFj/d2D2bURVxEsvKPY9XXcHIjr4Tt9ZEPo+l91y9l5C5ZKUvcVesJQtY6bDtA1XkJN78d0Vuqcq4NnDWenwORjdbmBkb9D/dYlj0fFwbCPIGetLllQmp1TYVIvnXAVdyjM/6+6bWpCoWatgclH63904ko4YiI07AALb4MJbWH+LZCzofrlCCb5mTD1ZF1OovP1+tITzwbWF11ugIiGsPzJ6pfRUiep+wo9oj4Mfh8GvXlgvEkoMhYGvAJ7lmj3uiqWsLjURfH7USAFMPI3HYD1RvMjoPtdsPZ/sPlb3Ve0X63IP0bo+MIxf8Nxs6HP0zp4+3M/SF9QefnKI2c9TB4JUqgNUWwXSDwZjv4TRs+DxFNg9SvwfUdIebf65Agme5bCr0Ngx28w8E0Y+Ko2VoEQ3VRfjrLpM8heW71yVpS8VFjxNEzsDL8MPvDkswB1dfnc2sBfZ8HmryFhGPR+FFqMqNj5u6bBgts03C2uNwwdp3Hy/ijKh9+GqSId/iksvF0blk7/gX7P6QxYN7tnwN/nQt4uXQq487XBbRD3bYHfjtA3PY36E5r09p1u9pU6oHjkD7qYWnkU7dfeT2TD4MnrDxHYOVndRNt/Umv8sK+hxZEVzyt3u05g63CpRlgFS768XdpgBNq4uM9LnQlr3oBNX4ArH5odDpnL1ZgY/mlg/4clqBx866HXBlwFugzBskcgdyu0GKWKvbwXbWT+A4vuVF9r/URdvjf5QggL8JX3Wavgp75QlKv+9sHvlYy48CQvVX3A23/SAeUOl0NsZ6jfznd5BVn64o/s1UAYNOkFjbqUDBXN3QG/Hwl5O9Qyjx/oX+aCHPj9cHUtHTPdt/IHyFkHU06A/ak6CcxX3YJBUT5s+hz+eV4Hdes1h843aONXlReUz7lWJ7Kdsr6s+6yi7E9X997mrzWENSYRGiZDg/bam6ufBCZce3guj61wr56zZzFENIIOl2jIb1wPNQimna7GQO9HofvdB0bv90Aiaw0suV+vW+vjg5q1VegHMkV5sOZNjUHev1vXUu92C5gIKMhWBVmYrd8zl8P6D3SlyB53Q9ebKhcfv+kr2P4L9H5clyfwh7jUF7z4Xh1cBnXPNOygirpRJ5Uxe7U2Fnk7y+YRFq1hoHG9dFv3niqFkb/6X+DMk31b1ZUhAsfN0sasNKmzNMpHCvX4nqVwyC3Q+wkNRfVH4T695mGR3pVT4T59I9XeDbplp8DGTzVqqXF3LSf5goq9JMUXOevg+y76//arwmqdO6fAzIv0PznkVq1bzgbYu16vf+42wM/zH9cbulynUVKlezuF+2D2VTru0vYMGDJOo6qqk5x1usR1foaz7dHPgj0QVg+63aYGRKjZ/TdMO1UXAgSdZ9Dved9vT6sgVVboxpjRwEtAOPCuiJQZtTHGnAOMRe+QxSJyvr88rUIvRUEOrH4ZVjyjN6g3wiLVPdLzgfIVcbDJS4WslZC9RpV39hq1xHNSdFygURfdYrs6n100/HDPEt0yFutn3g5Vekf+CC1HVkyGjCXw22FqWR7zl5brZtOX2puIaQNHTYIGSepSWv2qDogP/wwadSyZX1E+bP0OUt5Wlw6otRrRQLfwBirr/l3qsvAkLFJDYg+5RZd2CLaFOuNCHcc4ea1a/hXJ31WgSwksf0Ib3OGfQtP+ZdMV7dfeoYjWJywSjPMZFlW+sSACq17U69yoCwwbry97EZduuNQICIuG+m0qUnuPMnClUgEAACAASURBVFyw/TcdS9k2iRINUFiULn4X1URdVQVZ0P5i6PWw/v++ZN79t76gpn6iuoxiuwXv/9v4ub4HoUESHDEBNnwKK56A6GY6J6Tt6VUuokoK3RgTjr4g+hhgCzAXGCMiKzzSdAa+AEaKSIYxprmI7PKaoYNV6D7I36Ov0IuI0a5uZKxaPhGNNAIiUNfKgUreLlWagUR9eGP7r+pSaXk0HPm9WtUrn1E3VMIwOOK7ku6OzRP0lYCuQp0HkHyeWtdr34V176s89ZPUuo5sqK6Gwr1qgRbuhaJ9+jA2TIYGydCgnX7GtKred8buWabvpwW9XuH1nYbG+Yxp4/R4DtXP2EO0F5K9FmacD2lz9MUt/V6s/rGEnX/C9HPUzeWL+EEqT7vzSjbEvijIgnUfaIOcvVoXzev0H0g6C6ITIDKuZIOzP10V56pX9HfX/9NebFQT/Z2zTucErP9Qv4dFFkeZ1W+rjXLr0er6jGpc8WsgAiufhkV3QbPDVJm77/H0hXoPZixS1+WAV6o0H6aqCn0oMFZEjnN+363yyxMeaZ4GVotIwKEIVqFbKk3KO+oX7nilKtWUt/VF3UPHeXd57N2kSm7336oA9yxVJdnmZJ3Q0/LYA7Oh3DwBslYUNy7uBqZwL+zdqC44l7MqqImAxt3UlWIiYPA7qvxqin1bYev3jixhQJheYxOmLp/1H6q84fV1wbqOV2pIsTHaU8pJUZdd1ipNt2UCFOboTO8uN2pdvE3IKs3eTfqC+PUfQmRjnRmeOgt2/wUYaDFSrfi2Z0B+uroet/2kPbTCbJW51fHqvml+RGCWu6sQ5l2v92G782DI+2XvQ1eBGh5LH1IDbeCblf5/qqrQzwJGi8iVzu+LgMEicoNHmgmoFT8cdcuMFZGfveR1NXA1QFJSUv+NGzdWqkIWC4vuUYsMoMc90OsR/xazq1Afpm2TtNvb4bLKuwEOFFwF6vrKWFLs2gqvp1FJDdqGWrqSiGivYe27uvBdYQ406qz7965zXDQOMa2gxdHQ9cbyB8x9kbFEreXtP6kbsP0lGjzg67q4CjSiZ+uPOsazPxWaDoTut0Pi6d7XfyrK12u+5H7Y/nNg92HmCn0/QsfL1ZioBFVV6GcDx5VS6INE5EaPND8ABcA5QCLwF9BTRHw4g62Fbqki4oKlD6tSaH9BqKWxVISCHA2D3PSlWquNuqrSjT1Ex14CcckEyv40XVKhIj7ywn1q4a98TnsODdrrWEnLUbocd+psbZwyFoJrv1r1A98IbJ0o0FnEJqzSfnt/Cr2cZQcB9Zt7NmuJwDYvaWaJSAGw3hizCuiM+tstluBjwqDX2FBLYakMkQ3VQu14efWXVZmxmoj6OsGr41U6aL7iGZh/Y/Hx8Po6yNzlBh0baDa8Yr29anTvBaLQ5wKdjTHtga3AeUDpCJYJwBhgnDEmAegCrAumoBaLxVKjhIWrrz3xdF2xNGuVKvLGPcpfgjtElCuViBQaY24AfkH94++JyHJjzMPAPBGZ6Bw71hizAigCbheRtOoU3GKxWGoEY9QKD3TORAixE4ssFoulFuHPh173F+eyWCyWg4SQWejGmN1AZeMWEwA/sxjqNAdr3W29Dy5svX3TTkS8ThUPmUKvCsaYeb66HHWdg7Xutt4HF7belcO6XCwWi6WOYBW6xWKx1BFqq0J/O9QChJCDte623gcXtt6VoFb60C01izFmLNBJRC6spvyXA9eLyBRjjAHeA04D1gC3oks2dw1ymUnACqCxiHuhd4uldlNbLXRLkDHGnG+MmWeMyTHGbDfG/GSMOawmyhaRHiIyxfl5GLpUc6KIDBKRv4KhzI0xG4wxR3uUuUlEGlaXMjfKOmeyncVSI1iFbsEYcwvwIvA40AJIAl4HTg2BOO2ADSKyNwRlB5MjgOZAB2NMJZcMrBzGmANzXrql+hGRWrUBo4FVQApwV6jlqcZ6vgfsApZ57GsK/Ia6In4DmgShnMZADnC2nzRjgY89fn8J7AAygWlAD49jJ6CujGx07Z/bnP0JwA/AHiAdXZEzzDm2ATgauALIQ19L43Lq/wq6+Ju77uuBnWisbhrwqpNHR2Cysy8VGA/EOcc+cvLLdep6B5DslBPhpGkNTHRkSwGuKlX/L4APnXotBwYE8P+NB75xy1jqf3wfXeQuA10LqR4wx6lfLrAfWAtcAsxGVzOdAkSV/k886nIFsAmYFsD/FAM8h84FyQSmO/t+BG4sJe8S4LRqvt/DgYXAD87v9k691wCfu+tdlzbnvl8KLEKXUXHfG5V+xmuVhe68Pek14HigOzDGGNM9tFJVG+PQxsuTu4A/RKQz8Ifzu6oMRZXJtxU45yd0Nc3mwAJUcbn5H/AfEWkE9ESVLKgvfAvQDO0F3EOpF1qKyP+AO4FFIhKGKunT0DWH7nLyykLX3v8QaAN85pxugCdQxdwNXSF0rJPvRaiiO1nUzfK0lzp96sjXGjgLeNwYM8rj+ClOWXGo4n/V18UxxtR38hjvbOcZYzxfavoRUB/ogV7DF1AFfjv6QJ+OPuQ3oYvhvYA2jtmo0vbFkU7dj3N++/ufngX6A8OcMu9AG70PgH/HSowxvdHrPMlPucHgv8BKj99PAS8493oG/utdmxkhIn2kOPa8as94qFupCrZoQ4FfPH7fDdwdarmqsb7JlLTQVwGtnO+tgFVBKOMCYEc5acbiYaGXOhaHKubGzu9NwH+A2FLpHga+QwdXS+exATja+X4pMN3j2HRgt1P3k5zvieXVHW0IFnorw+PaCtpYtEUXlWvkcfwJYJxH/X/3ONYdyPVT9oWOnBFANNorOd3jf3PhxfIC3kKVd31UAQ9GexsRjvw3uu9/vFvoHfzI9O//hLpac4HeXtJFo72Uzs7vZ4HXq/k+T0SV10i0F2fc9XaOl3ju68rm/KcJpfZV6RmvVRY6ails9vi9xdl3sNBCRLYDOJ+VfzFhMWlAQqB+V2NMuDHmSWPMWmNMFnpTgrpUAM5E3S4bjTFTnVcYAjyDujJ+dQYLy7U8jDHJqIWZj1r19YGNIrKFUnU3xjQ3xnxmjNnqyPWxh0zl0RpIF5Fsj30bKXlv7fD4vg+o5+eaXQJ8ISKFIrIfdbtc4hxr65SV4eW8tsDZqKvpN9TlskdECp3jqfi/3/99Nsr5nxLQXtna0hk48n4BXGiMCUOXxf7IT5nB4EWKewgA8ZSsd119zgV9HuY7b3ODKj7jtU2he3vFh427rBozUb/1aQGmPx8dLD0atfaSnf0GQETmisip6I04AVUOiEi2iNwqIh2Ak4FbSrk0SmCMaQh8jbo23P/xZiDJhyJ9wknXS0RiUSvZ837xd59sA5oaYxp57EtC3RwVwhiTiFqaFxpjdhhjdqDulxOcdwVsdsqK83L6ZtTvnQgMQt0nbvaiPm53PVp6Od+zjv7+p1T0P+/ooxofoD23UcA+EZnpq75VxRhzErBLROZ77vaStC4+58NFpB/qQr7eGHNEVTOsbQo9kLcn1WV2GmNaATifu6qaoYhkAg8ArxljTjPG1DfGRBpjjnde/l2aRqi/Nw21mB93HzDGRBljLjDGNBZ9e1UW6srAGHOSMaaTE2fu3u8rZNCgynw8OngKOhC6CdiODpTuNsbUM8a4F6luhA547jHGtEH90Z7sBDr4uAabgRnAE06evVCf7Xhv6cvhItTH3xXo42xd0Ht3jGN1/QS8boxp4lxr94P8P+Ay1Lc9FTgWiHcasEWoP327MWYA2kj4w+f/JCIudND2eWNMa8eaH2qMiXaOz0St5eeofut8OHCKMWYDOkYxErXY4zwa7jr5nIvINudzFzqGNYiqPuOh9iNV0OcUgb4JqT0QBSzGY+S+rm2U9aE/gxPZgw6WPB3Esi4A5qGW4A402mGYc2wsxf7ahqgvPBt1S1yMWk+dnP/kZ3QQKwt929Vhznk3o93+vahyu9+j7A2U9KHvBF50fh/lpH/GqXMSGmWSi1qaLzvpegDzUaW+CGcQ1qOMU9EGYQ9wG2WjXBJR/2066oq4xuPcf+vv8b/8e26p6/gPpaJEnP13UDKS4QOnnhmoS6YZ6uc+HY18KESV2DRUkXdw0uc5/83LlPWhR3iU5/N/co7HoIpzK8VRMDEe599HOX75arjfj6I4yuVL4Dzn+5vAdaF+HoNc1wY4YzbO9xloEESVnvGQV6wSF+IE1AJaC9wbanmqsZ6fotZogaPQrkB9i3+gIU1/AE1DLWc11PswR5EscRTzIuc/r9N1B3qhYXtLgGXAA87+Dmg4Y4qj5KJrSJ6L8RicrqEyPRV6SOpdg3XtgBqki1ED5V5nf5Xuczv132KxlMAJu5yMRrd8GGp5LIFT23zoFoulGjHGHIeGXO4EPgmxOJYKYi10i8ViqSNYC91isVjqCCFbxCchIUGSk5NDVbzFYrHUSubPn58qPt4pGjKFnpyczLx580JVvMVisdRKjDEbfR2zLheLxWKpI1iFXptIXwh2ENtisfjAKvTawtYf4Od+sOnLUEtisVgOUKxCry2sekk/1wdhnseKp+H3o0Bc5Sa1WCy1B6vQq5uUt2HbL1XLI3Ml7Pgd6rWA7T9DXhXW5MrPhOWPwa6pulksljpD3VLo6QvAVVh+upoicwXMuQbmXgOuKryLePWrEBYFwz8FKYKNn1c+rzVvQEEWhMdAyruVz8disVQcETXOMv+pluzrjkLfuwl+7g8rnw21JMUseUA/926AbT9ULo/8TFj/AbQ7D1qMgCZ9YMPHlcurKA9WvQgtj4UOl8PmryHf23sWLBZLUCnIgdWvw489YPIxsPrlaimm7ij0TOd1hGtePzCs9PT5qjB73AP1E2HVK5XLZ904KNwLXW7U38kXQtocyFpdibw+gLyd0OMu6HQluPbD+sos+V0FVr0MP/WD/D01W24wKcyFP0bC3OtDLUlJlj4Mk3rDvgq/l+PAwVWo13bejXUjois7BebfDBMSYd71EFEfhnwA/Z6vluLqjkLPcd6mtW8zbPkutLIALL4PoppC9zug87Ww8w91wVQEcam7JX4IxDvvkG03BkwYbKigInYVwsqnIX4QND9KLf2m/WHtOzX34OzdCIvuhIyFsODmmikz2IjAvOtg559qPGyeEGqJlN0zYOlY2LME/jwW8lJDLVHlWPOGXtvVr0LKW6GWpvJk/gNTTobvu2hdWp8Ax8yA4+ZCh4shvF61FFt3FHp2ivqFGyTD6kpaw8Fi13QdvOx+F0TGQserICxa/9iKsP0XyEmBrjcW76vfGlqMUrdLRRTx5q8hZ53KZJw3fHW8UhVA+nz/5waLhbcDRt0968bB1h9rptxgsvYdlb37XdCkL8z9T+iVZ+E+mHUpNEiCI7/X/3nK8TpWUpvI261uyhajVAHO/z9InR1qqSpGUT4sexR+6g27p0PPB+C0TTD8E2g2tPjZqybqjkLPWQsNO0Dn6zR6I2NJaOQQgcX3QL2W0MXpktdrBsljNOQwPzPwvFa9ovm0LfW2seQL9aFNDfBVjyKw4kmI7QqJpxbvbzdGG8G1NTA4uvNPjaHvfjcMfB0a94Q5V9UuH37aXHUFtDoOej0KQ8ap/PNvLPfUamXxfZC9Bgb/D9qcBId9BRmLYOop6h6qLSy5DwqzYcDLMPQjiEmE6WdVLaqrJkmdreN4S+6HxNPgpJXQayzEtKoxEeqYQu8IHa9QJVVRazhYbP8Vdv8FPe9Tf5mbLjeqL3zd+4Hlk7UGtv8Ena+B8KiSx9qernUMdHB0+6/6gHe7U901bqIaQ9I5sOETla26cBXCvP/T3lO32yA8GoZ+oA/q/Juqr9xgkpcKf52pD+ew8RAWDk16QY/7YeNnsOnr0Mi1a7oOdHe+Flo679xucyIM/RB2TYPpZ4OrIDSyVYT0BZDyjj4njbtDdFM4/GvYnwp/n+d7XCxvN8y4EL5oBL8M1ftpw6dq8NSUK7EgR8v9dSgU7IEjJsJhn0OMt/d4Vy91Q6GLS//ARp30Rki+QJXd/vQalkPUymiQrG4WT5r2g4RhsPq1wCb0rHkNwiKh03/KHotspBbAxs+1i1ceK56EmDZ6XUrT8Uq1iqpzBuqaNyFzmQ4ERcTovqb9dMB4/Yew5fvqKzsYuIpgxhhtgA7/GqLji4/1uAua9IO516pyqUkK98Gsy6BBO+hT6n3eyWO0J7TtR5h5cdXCZqsbEXWvRCfAoQ8W72/aFwa+qb27JfeVPWf9R/BjN9j0BSSeDmEROu9jxvkwsSN80xymnKiDxdt+Dq4+yN2hY3WL7oFJPXWwv/N1cOJySDw5eOVUkJCtthhUcrdDUa5a6KCt/Np3Yd17ahHWFFsmQPo8GPJ+WavaLdeMMXpztTnBdz4FOWrJtz3bdyvf/iLY+Kla8Z5ulNKkzoJdU1SZepOp2XB1xax9Fzpc6q92lSMvVbugLY/WRsiTHvfpQzHnami2XBvjA5GlD2js8OB3dSDZk7BIGDpOu9rzblDLrKZYfI+OsYyaDJENyx7vfA0UZMIiZyxnwOvaszjQ2Pgp7P5br29UXMljHS5R1+KKp3RAv+0ZkLNe53fs+BUShsKgdyCuh6Z3FarxkDZHXSBps2HbT+hraoFGnTWf+EEatFARcrdrvmmzNfgCwERA/EDttTUbXqXLEAzqhkJ3R7i4FXqTXtD8CLWGu95cMzexq0gVV2xX9XF7I+lMWNhKB239KfT1H+qAVlc/vtmWx0B0M1j/sX+FvuIpiGpStsfgxhjocAUsukNDPxt3851XZXD7Rfu/VHZAKDxK/dC/DIL5/4VhHwW37GCwZSIsf1x7Mh2v8J4m7lDo+aDWddNZkHR2cMoW8T2Itusvxyq8Xucn+KL7nRoiuuJJXdxt8Lv6fBwoFOToYHnTAdDhMu9p+r+kLpmZl6rrcOVz6joc8Kq6mjzdiGERGsHVpA90utopI0sH/t0KfufkikeJuWnQXnvaCYMhfrAOjLt7nQcAdUOhZzsKvVGn4n1dblT/4bYfIfGU6pdh46eQuRwO+0JvKm+ERUKna2DpgxpHHtulbBoR9f83HaA3jC/CInRQM+UtfWBLWzagCnrLBB1p92bBuWl/sVp7a/8H/YI4MSt9oXaBu/5X/aLeaNpXXS/LHoaks/w3TjWJuFT2hbepVT6gnMip7nfClm9h7nXQ/Eio17zyZWetUZfbunEQ0UDvg/hBqkSaDlAFNusyde31ebL8/Ho/DnG91K3xc3+Vted91RY6VyGWPw6529SVZXx4gMOj4fCvVPZlj0DrE2HgG9CgbWBlRMZqo+du+EQgb0fFx40i46BeQsXOqWFC9k7RAQMGSNBecLH4PrVAzs1VpQna9ZrYHmIPgZG/BaccX7gK4Idu6tsePd/3jQnqe/suCTpdCwNeKnmsIEutriX36+SDDhf7Lzdtrlq3g98taz1mrtSIjNQZcOqm8m/Ev87UQbTTtnp3zVQUEfj9cG24Tl7tvcFxU5Sv9cjdphaXW4F5k7kwV620tNnq3opoVKzsYg/xf+0DJfMfjcDZPV1D6IZ+APXblH/enmWqdFoeqwooPDrwMsWlg9erXlY3WlgktD0TTLhaljkpTkKja/rk7YBRU6DFkYGXkZcKC2/VHmCjLjD4He3JlofbjZE6W10OeTu8p4toqI1f/GD99GdEgIYa/9hDDZOh48qXY89SnRHe+oRqD/87kDHGzBeRAd6O1RELPUUHhtzKHNSC7XwtLL63elwJnqx7X90+R/5QvkKJaamRJeveh96PaiOQtUqt8nXjoDAHWoyEdueUX27TAeriWf+RKnRXkfZIVr+iPt+waLXgArEqOl4Jm7+BrRPVUq4q6z/y7RctTXgUDPsYZl6kFpjb39mwoyrrJn10UlLabMhYDOJEPMS0UXdOypv6O6KR+jPjB0OjjoCXhz6igQ5iNupUVikU5auLavmjmm7I+9D+ksCVR1xP6PuchjH+eSwc/m354wJF+doTWP2yhh7WawmHjtXBcM/xk/1p2oCnOUq12eEVU+ag98HQD3RwfM5/4Pcj9X9PGFo2rbgg6x+n4ZyvY1SgA8INkvF6bfev1AFK0OegcQ/HLdFHo7JKs+Fj5x59IjD54w7VzeKTumGh/zxQ/cQjfy25P283TGirym7ga2XPExdgqtbaF+XBxE46qeOYvwPLK3U2/DpEH9q9G3USUlgUJJ2rfvP4gYGXv+xRteh7PqiW1971utRA52vVb17P66sHy+IqgonJ+hCO+Dnw8ktTkKPyrHpJ63HszIpZzQXZqkDciit1NuRuLams3RZ5TCtH8awqHqxKna2TpcRHmJubqKbFg2Pxg9X9MP//1G2WdK76bWNaVO4arB8Psy9XI+PIHyG2s/d0u2dqTyBzuSrVLjeqVR6MHlJ5FO6FJQ/Cqhd8R12FRWs0kvsaxQ/SuR7+7vG83R4Nj/Mf+ptr0P9l/2NFljL4s9ADUujGmNHAS0A48K6IlHHcGWPOAcai5tViETnfX55BVehfNdXFqwa+XvbYzEth81fqSijMKako0udp13DUn5VX6v+8AAtu0TxaHBX4eT8PgvS5qpQ6XasDOJVRIDnrYWIH/d7scOj6fxpN4suP748lD6qF3G6M5pPgx4fvjW2/6MzJvRu1Qen9hMa6V5X9aeq/DHRwuzAX9vsIIcxPdxSO0wBkLi9WaPUTNRIkGGFnu6bDX6ep6+mICdD88OJjBdnac1z9qpY58A2NHQ8F+9P1ufBGvZZVb1xENDrEWwMbFhWSWO3aTpUUujEmHFgNHANsAeYCY0RkhUeazsAXwEgRyTDGNBcRv9O7gqbQ96fD1/HQ91nodmvZ4+nz4ecBOjDingodFglxfbT7uP1nDfvyFyngi4IcVaZNelfcT5+1BrJWqD/Q01VUGbb9rA9Gkz5Vy6dwr45HrHtPr1XTgWo9JZ3j3x+cl6qN2oaP1I896B1ofljVZKkpCnL0Htm7QSdsRcYGL+/stTD1RJ0jMfg9aH+hLncw91rYtwW63AC9H1O3m8USIFVV6EOBsSJynPP7bgARecIjzdPAahEJeA550BS6e2Dw8G+h7Wne08y/WSeFuLvqTfpoF7soD75LhrjeMLISL6FY9piGqh07GxIGVakaBxQF2eoDX/2K+lHrNdfQxgbtvKTNhJXP6Gf3uzRi5UCInjhQyM+AaWfoXICEoRpT3bg7DHpX1/awWCpIVQdF2wCbPX5vAUr3xbs4Bf2NumXGikgZR6wx5mrgaoCkpKQAig4AbyGLpen/gvf94fXgkJt14kX6AvUXBkp+hiqyNqfULWUOajF2uU7dJjt+V8W+4kn+HawsTfxgHfyM61mjYtYKoprAiF/UKt/wMRz6kDZ8NeEntxx0BKLQvTmXSz/ZEUBn4CggEfjLGNNTREosei0ibwNvg1roFZbWG/9OKupQufM7XaOxsCueqtgsvxXPqFui1yOVK7c2YAy0Oka3/D3FkQ4lE2kY3UEcRlYu4VEw5H86MG97L5ZqJBCFvgXwjOBPBLZ5STNLRAqA9caYVaiCnxsUKf2Rk6IDi54LYVWEqMa6BsPKp9Wv7SsiwZPcnRrF0e68A2vWXXUSFQeUE35o8Y9V5pZqJpB4srlAZ2NMe2NMFHAeMLFUmgnACABjTALqglkXTEF9kr22eMp/Zen6XzCR8E+AsyRXPKFv+zn0oaqVa7FYLEGkXIUuIoXADcAvwErgCxFZbox52BjjnlP/C5BmjFkB/AncLiJp1SV0CXLW+vefB0JMS11HYt04DbHyx95N+laVDpcGZs1bLBZLDRHQjA8RmSQiXUSko4g85ux7QEQmOt9FRG4Rke4icqiIfFadQv9L4T6dLl5VCx10VUYphH9e9J9umeMz7/lA1cu0WCyWIFK710PPcbw6wVDojTpqvPWaN3y/wHjrDzplv9M1OjPUYrFYDiBquUIPIGSxInS/U9cGWfNGyf15u+DvMTD1ZIjtpivVWSwWywFG7Vbo2aXWQa8qTfpAq9H6Sq/CXJ22vO4DXUlx8zcaojh6fuDro1gsFksNUrtXW8xZq2t8BPNNN93vgj+OUl95+lydWNPsMJ3O3viQ4JVjsVhCxrZt0LQp1KtjkaS1W6FnpzjLpAaR5kdA/BANTYxopAt+dfpPcNbZtlhKkZcHn3wCp54K8fHlp68rLF8Ol1wCV1wB115bs+WOHQtffQWxsXD66TBmDIwaBRF+tKEIpKXBjh1lt6IiuPxyOPRAWNlXREKy9e/fX6rMdx1F/jq36vmUJnWOyJzrRPZuDn7eBwmbNomsWxdqKQ5s9u0TOfZYERBp3Vpk8uRQS6QsXy5y0UUiL74oUlQU/PznzxeJjxcJD9e633efiMtV+fyWLBG55x6Rr74SSU31nmbVKpHzzxcxRqRhQ5E77xS57DKR2FiVoXlzkeuvF5k+XWTtWpHvvxd56imRSy8VGTRIpFEjTVd6q1dPJDpav594oshff1W+HoECzBMferX2KvSiApFPIkQW3VO1fCxBZ+tWkZYtRaKiRJ57rnqUQm0nJ0dk5EhVMGPHinTtqt/vvlskPz80Mm3apEouLEwkMlK1w8iRuj9Y/P23KtGkJJF//hG5/HIt5+qrRQoLK57fnDkicXEllWzv3iI33SQycaLI0qVap/Bwkfr1Re64Q2T37uLzc3NFvvlG5OyzVTmXVtitWomMGiVyww3awH3xhcjUqdpAZGZqQ5SWJvLwwyIJCXrOsGEi331Xffd93VTo2WtFxiOS8r9KZ/HUU5W/kSzeycsTGTpUpEEDkdGjq0cp7N+vD+sVV4i88krVH5zNm1WpfvutKtrqJitL5IgjVHF+9JHuy8nR+oDI4MFqJQaL7dtF3n1X5NdfRbZsKWsNp6WJ3HabWppRUSI336xK79139X+MixP55JOqy/HHH5pfp04iGzfqPpdLrWsQOf10VbCBMmOGNg4dOoikpOjvrImMDQAAEVJJREFURx9VBeypnKOjVcHv2OE/v8xMkfHjRd55R/PKyKhY/fbu1fsxOVnL7d5dZNw4vV+DSd1U6Nt+UYW+Y0qlTt+5s7irdPPNVRPFUsx//qPX9Msv9WENllIoLBT580+Rq64SadJEy4iJqXqD8eOP2v33fPhPOEHkzTdV+VUUl8t/o5CZqRZceLjIp5+WPf7FFyKNG2sX/+OPK16+J3v3quXYoEFJqzM2VhuNyy4TueUWLc8YkYsvFtmwoWQeKSnaQIPImDEi6emVk+WHH/Ta9ughsm1b2eMvvaRlHHFEYIp02jR1nXTurA1yaXJzRaZMEXn11cr9j1WhoEAbhl69tE6JiSLPPy+SnR2c/OumQl/9uir0vZX7tx54QG/is87Sq/DWW1UTpy6wdq3I119X3uJ95x29lnfeWXK/p1I477yKKYX0dO0mt26t5zdoIHLBBaog9u+vfIORny9y++3ybxd92TL1Yd90k1p8buXXv79atoGQliYyZIjeV4MH6z02Y4Y+4CKqqAYNEomIUH+vLzZsEBk+vPh6+fIL+6KoSC3DNm00jzPPFFmwQBvE115TX/GIEeoWA5GTTlI/tC8KCkQeeUTlTkzUXsWGDYH7vb/8Ul04/fqVdHeU5tNPNV2vXiKrV/tO9+ef6j455BB17x2ouFwikyaJHHmkXucmTUTuv19k166q5Vs3Ffr8W0U+qyfiqrj22btXrbJTTtGb9fjj9Wb944+qiVQb2b9frcKjjy5WYtddV/FBqlmztLt+7LHeXVgFBdodjohQv+RbbxUrOm+4XPqAt2ihrolTThH57DPv1m9FG4yNG4vTX3NN2W6+y6UDg088IdKli1rT5TX4W7eq9RkdLfLf/2r+YWFaRlyc+mj79FGFNWGC/7xE9No8/LBer5YtdZAuECZPFunbV8sdNKj8Qbq8vMDyFRGZO1d9/e77pFkzHQgcO1YV17JlKueLL6rPefRoda8Yo72SPXvKL+PXX4t7FF276rWcNEmfWRGR337TnlmPHuW7UA4kZs4UOe204p7l889XPq+6qdCnniryQ/dKnfrGG1rzadP0d2am3iBxcTrY4Y09e9TyTEzU7lR1sXmzWrqBWslut8bff1esnFWr1EJt1kyvRVKSyEMP6QME+hmoUt+xQ63B5OTyrcl58/ThBlWWX31Vtpx164r97wMGqHVZHm4rMjxcZXn7bXWnzJihg2+7dmma775TS6lRI5HPPy8/36ysYlnuuMP7/5KSItK+vboAPCNV0tK0jMsv1x5GgwYqU0VYsEDk0EO1/Esv9a4Ud+xQ3627kUpK0t5KdQzK5eerYn/tNZWne3dV2KUHE2Nj1SI/5xy9ryribtiwQeSFF/S6u33h0dHqG4+OVgu+qlZuqFi5Uu+Hr7+ufB51U6H/0FNkyikVPq2wUP1ugwaVVCTr1qly69xZH0Q3BQV687pHsDt2LH64gz2Yundvsd/tqacCO+ett4ofolNOUSvJF0VFau24rfHwcB2ImjSpuC4uV7FSv+OO8pV6fr7I4Yer1bFwYWAyu1xqpXbvXmxJTp6seT39tObVsKH6VSt6jefMKWlFetv691clHCgFBWrJg7ro9u0rPrZ0qVrQTZuKzJ7tv86VjV7Zv1/k3nv1/0pMFPnlF1Xs77+vPSJ3T6BXL40q8pSvJsjMVDfI+PFqie7aVbUwRE/27dP63nyz3i+jRlXcBVXXqHsK3eUS+ay+yLyKj2Z++63W2pt1Nn26ug1GjNCH6Pvv1U8HIkcdpfGz+fki116r+44/vuIj4b5wuTT21xiRgQP14Z0xw/85S5aoBXPMMSKPPaZWUViYWk7uKAIRbSjefLO4Lq1bq/vD2+CUWxZ3He+/33f5u3frwBpUrtdSWCjy3nsibdtqHi1a6Oepp1YtKiY/X3sgs2ZpY/XxxyIvv6yugeefr5ibwY3LJfLss/r/DBmig+ozZ6q137q1/4Y0WMyZI9Ktm16jqCj9bN9elX1NlG85MKh7Cn3fNh0QXfVqhU8dPlxdA778tx99pFfFrWS6dFFrsrTF8eab6t/s2lW79L4I1MJ8/XUt76GHtJFITtausy9/cE6OKuiWLYt9iampIrfeqt3S6GiNYLj33uIojn79VLkFEkZVVCRy5ZV63sMPl9z/xx/qq3YrlTvuCKyOvsjNVWU5cqTGBB/IfPWVNqLt2qkLpWPHmp1AlZur/8fNN2uDFSxL2FJ7qHsKfec0Vehbf6rQaTNnao1fesl/uocfVkX58sv+u8nTpqmbpnFjtQTT0tR18PzzGgLWq5cOgg0b5j+ueOZMTXfCCcV+z9mztcE47TTvD+1ll6m1+PvvZY9t3Fg8QcQYzWPq1Io//EVFWg9Q6/bJJ4tdTk2aiPzf/6nL4WBj1iydWXjoob57ORZLdVH3FPra91WhZ62p0GlnnaUDn8GKBxXRAZw+fcTrDLPRo1Xp+Ysr3rlTB/Haty9rjT/7rOb1yisl97t7Ef7cISJqOZaOK64ohYUaf+yu1xFHaPk17ac90MjJCd2MTsvBTd1T6IvuE/kkXKQo8CcqJUUt1rvuqnyxvsjJ0QiLp5/WAZzS4VSeccUXXaSREyLq9hkxQrvw3iI5ioo0LCwqSv33IuobbtBAByL9hf0Fk4ICjWteubJmyrNYLL6pewp9+hiRCe0rdMoNN6hbI1QTEQoK1G0RFqZui9mzNQwSVFn6YvduteA7ddLogd691SfubXacxWKp+/hT6LVz+dzslAq9pSg9Hd57D84/H1q3rka5/BARAQ8+qMt0XnABDB8OhYVwzTW6jKgvEhJ0edURI6BnT9i1C374ARITa052i8VSOwhokW9jzGhjzCpjTIox5v/bu9cYqco7juPfHytoY4kKrASBFms2UdJQTAnBC2TRloCXxQtNMCVR08Y3NdpI02BNbGrji/ZF9Q2Jsa2pMW3VjJduDYnVQWwjqbBUqFwjGCwrxt1WbSWkblb/fXHO6rgddgd2z5w9Z36fhJx5nnl25v+Es789eebMORvqPH+LpH5JO9N/3x3/UmscO3RSdyl66CE4fhzWr8+wpgZdfjns3Alr18JVV8GDo9yTGmDZsuQazn19cNddcPXVmZdpZgU06hG6pDZgI/BNoBfYLqk7IvYOG/pERNyeQY2fN/ABDLzX8I0tPv4YNm6EFSsmyAXogXPOgcceO7mfuece6OyESy7JpCQzK4FGjtAXAwcj4s2IGAAeB1ZnW9YIjp3cfUS3bk1uN3XrrRnW1ASTJsHSpSPfVcXMWlsjgT4bOFLT7k37hrtR0t8lVSTNrfdCkm6T1COpp7+//xTKJVk/h4bX0CsVOP10L1OYWfk1Euiq0xfD2n8E5kXEAuBF4NF6LxQRD0fEoohY1N7efnKVDpl5JXRugqkdow795BN4+mlYuRKmTj21tzMzK4pGAr0XqD3ingMcrR0QEf+KiI/S5i+Br49PeXWcMQPOWwVto9+ue9s26O2FNWsyq8bMbMJoJNC3Ax2Szpc0BVgLdNcOkDSrptkF7Bu/Ek9dpQKTJ8O11+ZdiZlZ9kb9iC0iBiXdDjwPtAGPRMQeSfeRnODeDdwhqQsYBN4Dbsmw5oZEJIG+YgWcdVbe1ZiZZa+hcyYiYhOwaVjfvTWP7wbuHt/SxmbHDnjrreT8bTOzVtDQF4uKqFJJTvHr6sq7EjOz5ihloA8tt1x5JUyblnc1ZmbNUcpA37ULDh3y2S1m1lpKGeiVCrS1wXXX5V2JmVnzlC7Qh5ZbOjuTKxWambWK0gX63r1w4ICXW8ys9ZQu0CsVkOD66/OuxMysuUoZ6EuXwsyZeVdiZtZcpQr0/fth924vt5hZaypVoD/1VLK94YZ86zAzy0OpAr1SgUsvhdn1rtZuZlZypQn0w4eTe3XeeGPelZiZ5aM0gf7ii8l21ap86zAzy0tpAr1ahVmz4MIL867EzCwfpQj0CNi8ObkYl+rdMM/MrAWUItB374a+viTQzcxaVSkCvVpNtg50M2tlpQn0jg6YO3f0sWZmZVX4QB8chJdf9tG5mVnhA337dvjwQwe6mVlDgS5ppaQDkg5K2jDCuDWSQtKi8StxZNVqcmbL8uXNekczs4lp1ECX1AZsBFYB84GbJM2vM24qcAfw6ngXOZJqFRYuhOnTm/muZmYTTyNH6IuBgxHxZkQMAI8Dq+uM+ynwc+C/41jfiI4fh61bvdxiZgaNBfps4EhNuzft+5Ski4G5EfHcSC8k6TZJPZJ6+vv7T7rY4V55BQYGHOhmZtBYoNf77mV8+qQ0CXgAWD/aC0XEwxGxKCIWtbe3N17lCVSrMHlyckMLM7NW10ig9wK1Z3jPAY7WtKcCXwW2SDoMLAG6m/HBaLUKS5bAmWdm/U5mZhNfI4G+HeiQdL6kKcBaoHvoyYj4d0TMiIh5ETEP+CvQFRE9mVScev992LHDyy1mZkNGDfSIGARuB54H9gFPRsQeSfdJ6sq6wBPZsiW5KJcD3cwscVojgyJiE7BpWN+9JxjbOfayRletJkstixc3493MzCa+wn5TtFqFZctgypS8KzEzmxgKGehvvw3793u5xcysViEDffPmZOtANzP7TCEDvVqFGTNgwYK8KzEzmzgKF+gRSaAvXw6TCle9mVl2CheJb7wBvb1ebjEzG65wgb5tW7J1oJuZfV7hAn3duuQslwsuyLsSM7OJpaEvFk00552XdwVmZhNP4Y7QzcysPge6mVlJKCJGH5XFG0v9wFun+OMzgH+OYzlF0qpz97xbi+d9Yl+OiLo3lMgt0MdCUk9ENO1G1BNJq87d824tnvep8ZKLmVlJONDNzEqiqIH+cN4F5KhV5+55txbP+xQUcg3dzMz+X1GP0M3MbBgHuplZSRQu0CWtlHRA0kFJG/KuJyuSHpHUJ2l3Td80SS9IeiPdnpNnjVmQNFfSS5L2Sdoj6c60v9Rzl3SGpG2SdqXz/knaf76kV9N5PyGplDddlNQm6TVJz6Xt0s9b0mFJr0vaKakn7RvTfl6oQJfUBmwEVgHzgZskzc+3qsz8Blg5rG8DUI2IDqCatstmEFgfERcBS4Dvpf/HZZ/7R8AVEfE1YCGwUtIS4GfAA+m83we+k2ONWboT2FfTbpV5L4+IhTXnno9pPy9UoAOLgYMR8WZEDACPA6tzrikTEfFn4L1h3auBR9PHjwLXNbWoJoiIdyLib+njD0l+yWdT8rlH4ljanJz+C+AKoJL2l27eAJLmAFcDv0rbogXmfQJj2s+LFuizgSM17d60r1XMjIh3IAk+4Nyc68mUpHnAxcCrtMDc02WHnUAf8AJwCPggIgbTIWXd3x8Efgh8kran0xrzDuBPknZIui3tG9N+XrTL56pOn8+7LCFJXwSeAr4fEf9JDtrKLSI+BhZKOht4Brio3rDmVpUtSdcAfRGxQ1LnUHedoaWad+qyiDgq6VzgBUn7x/qCRTtC7wXm1rTnAEdzqiUP70qaBZBu+3KuJxOSJpOE+W8j4um0uyXmDhARHwBbSD5DOFvS0IFXGff3y4AuSYdJllCvIDliL/u8iYij6baP5A/4Ysa4nxct0LcDHekn4FOAtUB3zjU1Uzdwc/r4ZuAPOdaSiXT99NfAvoj4Rc1TpZ67pPb0yBxJXwC+QfL5wUvAmnRY6eYdEXdHxJyImEfy+7w5Ir5Nyect6UxJU4ceAyuA3YxxPy/cN0UlXUXyF7wNeCQi7s+5pExI+j3QSXI5zXeBHwPPAk8CXwL+AXwrIoZ/cFpoki4H/gK8zmdrqj8iWUcv7dwlLSD5EKyN5EDryYi4T9JXSI5cpwGvAesi4qP8Ks1OuuTyg4i4puzzTuf3TNo8DfhdRNwvaTpj2M8LF+hmZlZf0ZZczMzsBBzoZmYl4UA3MysJB7qZWUk40M3MSsKBbmZWEg50M7OS+B/trL9eK33BfAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "# plot loss\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Cross Entropy Loss')\n",
    "pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "# plot accuracy\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Classification Accuracy')\n",
    "pyplot.plot(history.history['acc'], color='blue', label='train')   # 'accuracy'\n",
    "pyplot.plot(history.history['val_acc'], color='orange', label='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 200, 200, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 100, 100, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 100, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 100, 100, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 50, 50, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50, 50, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 50, 50, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 40000)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                2560064   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 2,588,323\n",
      "Trainable params: 2,588,323\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "## CHANGE INPUT SHAPE, BASED ON RGB OR GRAYSCALE\n",
    "# specify padding or not?\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#model.add(Dense(1, activation='sigmoid'))  ## CHANGE HERE, BASED ON NO. OF CLASSES\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# compile model\n",
    "opt = SGD(lr=0.001, momentum=0.9)\n",
    "#model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])  ## CHANGE HERE, BASED ON NO. OF CLASSES\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-01-03 18:47:43'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "32/32 [==============================] - 109s 3s/step - loss: 1.2609 - acc: 0.3275 - val_loss: 1.0982 - val_acc: 0.3699\n",
      "Epoch 2/20\n",
      "32/32 [==============================] - 107s 3s/step - loss: 1.0987 - acc: 0.3415 - val_loss: 1.0985 - val_acc: 0.3653\n",
      "Epoch 3/20\n",
      "32/32 [==============================] - 106s 3s/step - loss: 1.0984 - acc: 0.3410 - val_loss: 1.0988 - val_acc: 0.3653\n",
      "Epoch 4/20\n",
      "32/32 [==============================] - 105s 3s/step - loss: 1.0976 - acc: 0.3536 - val_loss: 1.0998 - val_acc: 0.3607\n",
      "Epoch 5/20\n",
      "32/32 [==============================] - 105s 3s/step - loss: 1.0990 - acc: 0.3379 - val_loss: 1.1002 - val_acc: 0.2877\n",
      "Epoch 6/20\n",
      "32/32 [==============================] - 116s 4s/step - loss: 1.0979 - acc: 0.3496 - val_loss: 1.0985 - val_acc: 0.3014\n",
      "Epoch 7/20\n",
      "32/32 [==============================] - 122s 4s/step - loss: 1.0978 - acc: 0.3437 - val_loss: 1.1011 - val_acc: 0.2785\n",
      "Epoch 8/20\n",
      "32/32 [==============================] - 108s 3s/step - loss: 1.0978 - acc: 0.3538 - val_loss: 1.0995 - val_acc: 0.2922\n",
      "Epoch 9/20\n",
      "32/32 [==============================] - 104s 3s/step - loss: 1.0972 - acc: 0.3582 - val_loss: 1.1013 - val_acc: 0.2877\n",
      "Epoch 10/20\n",
      "32/32 [==============================] - 108s 3s/step - loss: 1.0978 - acc: 0.3384 - val_loss: 1.0999 - val_acc: 0.2831\n",
      "Epoch 11/20\n",
      "32/32 [==============================] - 120s 4s/step - loss: 1.0978 - acc: 0.3428 - val_loss: 1.1006 - val_acc: 0.2968\n",
      "Epoch 12/20\n",
      "32/32 [==============================] - 110s 3s/step - loss: 1.0979 - acc: 0.3467 - val_loss: 1.1019 - val_acc: 0.2740\n",
      "Epoch 13/20\n",
      "32/32 [==============================] - 128s 4s/step - loss: 1.0977 - acc: 0.3501 - val_loss: 1.1009 - val_acc: 0.2968\n",
      "Epoch 14/20\n",
      "32/32 [==============================] - 123s 4s/step - loss: 1.0970 - acc: 0.3523 - val_loss: 1.0992 - val_acc: 0.2968\n",
      "Epoch 15/20\n",
      "32/32 [==============================] - 121s 4s/step - loss: 1.0985 - acc: 0.3400 - val_loss: 1.1036 - val_acc: 0.2648\n",
      "Epoch 16/20\n",
      "32/32 [==============================] - 127s 4s/step - loss: 1.0979 - acc: 0.3457 - val_loss: 1.1013 - val_acc: 0.2831\n",
      "Epoch 17/20\n",
      "32/32 [==============================] - 116s 4s/step - loss: 1.0967 - acc: 0.3504 - val_loss: 1.1007 - val_acc: 0.3105\n",
      "Epoch 18/20\n",
      "32/32 [==============================] - 110s 3s/step - loss: 1.0975 - acc: 0.3426 - val_loss: 1.1001 - val_acc: 0.2785\n",
      "Epoch 19/20\n",
      "32/32 [==============================] - 113s 4s/step - loss: 1.0977 - acc: 0.3524 - val_loss: 1.1017 - val_acc: 0.3014\n",
      "Epoch 20/20\n",
      "32/32 [==============================] - 110s 3s/step - loss: 1.0980 - acc: 0.3427 - val_loss: 1.1009 - val_acc: 0.2877\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "history = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n",
    "                              validation_data=test_it, validation_steps=len(test_it), epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-01-03 19:25:33'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d285adad68>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO2deZgcRfnHP+/eu7nvC0IIhCOQgyREICRAuEJEoojKISKiyE9QEFFQEBBEQBRRQRERAeWSS8J9k3AFswk5IQkhBHLf2exmr9mZ9/fH28NONrO7s7szO7uz7+d56umeruqut2u6v139VnWVqCqO4zhO5pKVbgMcx3Gc1OJC7ziOk+G40DuO42Q4LvSO4zgZjgu94zhOhuNC7ziOk+G40DuO42Q4LvRO0hGRM0WkWETKRGSdiDwvIkem0Z57RaQ6sCca5ie477Ui8u9U25goIrJSRI5Ltx1O+8KF3kkqInIpcBvwG6AfMBj4CzCtnvQ5rWTab1W1c0wYlYyDiuH3kdOm8QvUSRoi0g24DrhQVZ9Q1Z2qGlLVp1X1p0Gaa0XkMRH5t4jsAL4tIvkicpuIrA3CbSKSH6TvLSLPiMh2EdkqIm9GhVVELheRNSJSKiJLReTYZtg8RERURM4Rkc9EZLOIXBnETQF+AXwj9i1ARN4QkRtE5G2gHBgqIgNFZHpg43IR+V5MHtFzfiSwda6IjArifioij9ex6c8iclszzuV7Qd5bA1sGBttFRP4gIhtFpEREFojIwUHcVBH5ILBrjYhc1tR8nXaAqnrwkJQATAFqgJwG0lwLhIAvYxWNQuzhMAvoC/QB3gGuD9LfCNwJ5AZhIiDA/sAqYGCQbgiwTz153gv8up64IYACfw9sGQVUAQfG2PvvOvu8AXwGHATkBHbNwN5cCoDRwCbg2DrnfFqQ9jLgk2B9ALAT6B6kzQE2AmPrsXclcFyc7ZOBzcAYIB/4MzAziDsRmAN0D8ruQGBAELcOmBis9wDGpPs68pD84DV6J5n0Ajarak0j6d5V1f+qakRVK4CzgOtUdaOqbgJ+BZwdpA1hYriX2tvBm2qqFMYEbbiI5KrqSlX9uIE8LwveCqLhvjrxv1LVClWdD8zHBL8h7lXVxcG59geOBC5X1UpVnQfcHXMOAHNU9TFVDQG3Yg+Ew1R1HTAT+FqQbgpWhnMayb8uZwH3qOpcVa0Cfg4cLiJDsDLsAhwAiKp+GORLEDdcRLqq6jZVndvEfJ12gAu9k0y2AL0T8LuvqvN7IPBpzO9Pg20AtwDLgZdEZIWIXAGgqsuBS7Da8kYReTjqqqiH36lq95hwTp349THr5UDnJpzDQGCrqpbWOYdB8dKragRYHXOO9wHfDNa/CfyrkbzjsUsZqmoZ9n8MUtXXgNuBO4ANInKXiHQNkn4VmAp8KiIzROTwZuTttHFc6J1k8i5QibllGqLukKlrgb1ifg8OtqGqpar6E1UdCnwJuDTqi1fVB1X1yGBfBW5u+Sk0amu87WuBniLSJWbbYGBNzO89oytBG8MewX4A/wVGBn7zk4EHmmHnLmUoIp2wN6w1AKr6J1Udi7mb9gN+GmyfrarTMLfZf4H/NCNvp43jQu8kDVUtAa4G7hCRL4tIkYjkishJIvLbBnZ9CLhKRPqISO/gGP8GEJGTRWRfERFgB+ayCYvI/iIyOWi0rQQqgrhkswEY0lDPGlVdhbUr3CgiBSIyEjiPXQV7rIicGrztXIK1A8wK9q8EHgMeBP6nqp81YlNukE805AT7nisio4My+Q3wnqquFJFDReQLIpKLtQdUYmWYJyJniUi3wKUULV8nw3Chd5KKqt4KXApchTVIrgIuwmqL9fFroBhYACwE5gbbAIYBrwBl2BvDX1T1Dcw/fxPWALkeq5H+ooE8fia79qPfnOApPRost4hIQ/7rM7CG3bXAk8A1qvpyTPxTwDeAbZjv/tRAXKPcB4wgMbfNc9iDLRquVdVXgV8Cj2MNrPsApwfpu2KNzdsw984W4HdB3NnAyqAH1AXUupCcDEKsXctxnFQhItcC+6pqvSIqIoOBJUB/Vd3RWrY5HQOv0TtOmgncQpcCD7vIO6mgtb5KdBwnDkGj6QbMpTIlzeY4GYq7bhzHcTIcd904juNkOG3SddO7d28dMmRIus1wHMdpN8yZM2ezqvaJF9cmhX7IkCEUFxen2wzHcZx2g4h8Wl+cu24cx3EynIwTem9bdhzH2ZWMEfpQCPbbD264Id2WOI7jtC0yRuhzc602P29eui1xHMdpW2SM0AOMGgULFqTbCsdxnLZFRgn9yJGwfDns3JluSxzHcdoOGSf0qrB4cbotcRzHaTs0KvQick8wqfCieuLPCiYbXiAi70QnPQ7iVorIQhGZJyIp7xg/cqQt589PdU6O4zjth0Rq9PfS8GBLnwBHqepI4Hrgrjrxx6jqaFUd1zwTE2fIEOjc2f30juM4sTT6ZayqzgwmGK4v/p2Yn7OwKdLSQlaW1epd6B3HcWpJto/+POD5mN+KTeo8R0TOb2hHETlfRIpFpHjTpk3NNiAq9P7hlOM4jpE0oReRYzChvzxm8wRVHQOcBFwoIpPq219V71LVcao6rk+fuOPyJMTIkbB9O6xa1exDOI7jZBRJEfpgMuS7gWmquiW6XVXXBsuN2Dya45ORX0NEG2TdfeM4jmO0WOiDuS6fAM5W1WUx2zuJSJfoOnACELfnTjIZMcKWLvSO4zhGo42xIvIQcDTQW0RWA9cAuQCqeidwNdAL+IuIANQEPWz6AU8G23KAB1X1hRScwy507Qp77+1C7ziOEyWRXjdnNBL/XeC7cbavAEbtvkfq8Z43juM4tWTUl7FRRo6EpUuhoiLdljiO46SfjBX6SAQ++CDdljiO46SfjBT6UYHDyN03juM4GSr0Q4dCUZELveM4DmSo0Gdnw8EH++BmjuM4kKFCDz4UguM4TpSMFvotW2DdunRb4jiOk14yVui9QdZxHMfIWKH3oRAcx3GMjBX6Hj1gzz29QdZxHCdjhR58KATHcRzoAEK/ZAlUVaXbEsdxnPSR0UI/ahTU1JjYO47jdFQyWuh9EhLHcZwMF/phwyA/3xtkHcfp2GS00OfkwEEHeY3ecZyOTUYLPZif3oXecZyOTMYL/ciRsGGDBcdxnI5Io0IvIveIyEYRiTuxt4icJSILgvCOiIyKiZsiIktFZLmIXJFMwxPFG2Qdx+noJFKjvxeY0kD8J8BRqjoSuB64C0BEsoE7gJOA4cAZIjK8RdY2Ax8KwXGcjk6jQq+qM4GtDcS/o6rbgp+zgD2C9fHAclVdoarVwMPAtBba22T69IEBA1zoHcfpuCTbR38e8HywPghYFRO3OtgWFxE5X0SKRaR406ZNSTXKG2Qdx+nIJE3oReQYTOgvj26Kk6zeaUBU9S5VHaeq4/r06ZMsswDz03/wAYRCST2s4zhOuyApQi8iI4G7gWmquiXYvBrYMybZHsDaZOTXVEaOhOpqWLo0Hbk7juOklxYLvYgMBp4AzlbVZTFRs4FhIrK3iOQBpwPTW5pfc/CeN47jdGQS6V75EPAusL+IrBaR80TkAhG5IEhyNdAL+IuIzBORYgBVrQEuAl4EPgT+o6qLU3IWjbD//pCb60LvOE7HJKexBKp6RiPx3wW+W0/cc8BzzTMteeTlwfDhLvSO43RMMv7L2CgjR/rgZo7jdEw6lNCvXQubN6fbEsdxnNalQwk9wMKF6bXDcRyntekwQj8qGIHH/fSO43Q0OozQ9+sHffu60DuO0/HoMEIP3iDrOE7HpMMJ/eLFNmG44zhOR6HDCX1lJSxfnm5LHMdxWo8OJfTeIOs4TkekQwn9gQdCdrYLveM4HYsOJfT5+XDAAd4g6zhOx6JDCT2Yn95r9I7jdCQ6pNB/9hls355uSxzHcVqHDif00QZZHwrBcZyOQocT+uiYN+6ndxyno9DhhH7gQOjZ0/30juN0HDqc0It4g6zjOB2LDif0YEK/cCFEIum2xHEcJ/UkMmfsPSKyUUQW1RN/gIi8KyJVInJZnbiVIrIwdi7ZtsCoUVBeDitWpNsSx3Gc1JNIjf5eYEoD8VuBHwG/qyf+GFUdrarjmmhbyvAGWcdxOhKNCr2qzsTEvL74jao6Gwgl07BUMnw4ZGW5n95xnI5Bqn30CrwkInNE5PyGEorI+SJSLCLFmzZtSqlRRUUwbJgLveM4HYNUC/0EVR0DnARcKCKT6kuoqnep6jhVHdenT58Um2V+ehd6x3E6AikVelVdGyw3Ak8C41OZX1MYOdIaY3fsSLcljuM4qSVlQi8inUSkS3QdOAGI23MnHUQbZBe1GYscx3FSQ05jCUTkIeBooLeIrAauAXIBVPVOEekPFANdgYiIXAIMB3oDT4pINJ8HVfWFVJxEc4gK/YIFcMQR6bXFcRwnlTQq9Kp6RiPx64E94kTtAEY1066UM3gwdOvmfnrHcTKfDvllLPhQCI7jdBw6rNBDrdD7UAiO42QyHV7oS0vh00/TbYnjOE7q6PBCD+6+cRwns+nQQn/wweard6F3HCeT6dBC37kz7LOPC73jOJlNhxZ6MPeNj2LpOE4m40I/EpYvh507022J4zhOanChHwmqsHhxui1xHMdJDR1e6EcF3+66n95xnEylwwv9kCHWKOt+esdxMpUOL/RZWTBihNfoHcfJXDq80EPtUAiq6bbEcRwn+bjQY0K/fTusXp1uSxzHcZKPCz3eIOs4TmbjQo8NhQDeIOs4TmbiQo9NQDJkiNfoHcfJTFzoA3wSEsdxMpVE5oy9BzgZ2KiqB8eJPwD4JzAGuFJVfxcTNwX4I5AN3K2qNyXL8GQzciQ88wxUVkJBQbqtcZw2jipUboDSZVD6kYUdy6BmJ+T3gvzetsyLWY9d5hSl+ww6FI0KPXAvcDtwfz3xW4EfAV+O3Sgi2cAdwPHAamC2iExX1Q+abW0KGTXKZppavBjGjk23Ne0AVbupKzdA5UZbVm20uJwukNsFcrsG613td04XyC6wsaGbm19oO1SXxCxLdt2GQE7nIL/Ou65/vq0L5HaG7KKm26IKWgORaghX2TJSBeHossLsjIZw+a6/d4mr+7vcjp9dEITCpi9zOgXl3a025HWDrNymlzlA1dZAyJfVinlU2GtKa9Nl5ULnfSzvso+harP9N/WRXWCCn9dr14dAbleQXDteNDT1d+w2yak/LisXJI5T4/NrbYedY2gHhErr/I63rdSuh+x8yMrffRlvW91lblcY9MXm/VcNkMjk4DNFZEgD8RuBjSJS17rxwHJVXQEgIg8D04C2JfSqgDJyRITc7AhvvBqma1GYnKww2VlhsrMjZGeFyZLgd7DM+nw9goitC+FAACrthg9XWohUQk2FLaPbYuNjf2sINAyRGltq2ITl8/UwGqlBI2E0HLMesXQRKSCS3Q3N7ormdkXyupGV15Wsgm7kFHYlu6BbjBB0rbPezeyICnesgMf+ji7DFU0vb8mp8xCos65hE4jqOgIe2mFxDREVs0goUWN2fyhItgl2XSGP/U0LPrjILjIxrhsKB1ktV7J2vR6qt0F4XZ3rJXqt1DQh34LdxT/eA6FmZ4yYL4PqrTHFlQWd9oYuw6DPBOiyn6133Q+KBkNW9q55Rmps/6rNULUlCJuhesvu27bPt/VQaXAPtNb8nrLrA4CI2ZDIfyw5tZWY6DWclRc8tLcGD/6q4PqJWQ9X1X/8gv5w6roknp+RSI2+uQwCVsX8Xg18ob7EInI+cD7A4MGDm5fj9H2tkInYhRIN1LOuYaIFvh9QHX1neb952SdKOJJFdbiQUKSAUKSQGi2gRgsIU0BNOJdQOIdQTTahmjyqQzlUh7I/D5XVOVRVZxOOZFMTziEcySastesFuZV0Kyqha+EOuhbuoFvRalsWlpBfWNZMe7PZXtmX7ZX92F7Zl5Kq/dhe2Y+Syr5sr7JlSZUFJYvCnFKKckspyttBYU4pnXJ3UJhbatuDbYW5wTKnlMLcbRTmfEZhzg6UbMprulMZ7kZleE+q9GCqIt2o1u5U040Q3amRbtRkdaNGuhPO7kYkpzuR7G5k5RYAQqSmGqkpQ8KlZEXKyI4ESy0lW8vI0TJyKCWXMnLFlnlZpeRllYIqNZE8QpF8W4Zrl6GY39XRZY1trwnbtupwAZWhTlTWdKKiphNVNZ2oCH5X1RQSUatBRj/Oq7sUgdxcyMmpP0Tj83JqKMirpCC3goLcSgpyKsjP2UlhdgkF0ZBVQn72DvKzSsiXks+XeVJCnqwjT0rIpYQ8qa2dl0X2ZGvNMDZXfp315cNYWzqMVdv347Ote1O6M4/ycuoNNTVmn9mYQ25uX3Jz+35ud9T2+tbz86GoCDoVRejcKUSXohCdO1XTqTD0eSgqDFFUYKEwP0RBfojCPFtmEaK6KkQoCDXVNYSqQ9RUh6gJhQhXhwjXhAiHQkRqaojUhIiEQ2jYluFwFuU1XakIdaEiWO4MdWVnVVfKQ10or+7Kzuou7KzuSlVNPuGwEInweYj9H+sLoORk1ZCXXUVuThV52RZys6vo3i3Cr05t1m3aIKkU+njvxPU+JlX1LuAugHHjxjWvyjTgRKvNSbbVPiQLyKpdr/ubrF3Srl0nbNpsIhqOZBPRbCKRrM/FNBzJJhzOpiYSkyaSZb+D7ZXV+ZRXF1BeWUB5VQFlFYXsrCygrKKAsvICSssLKa/MoaqKuCEvz8be6dwZOnWqXd9lW49dt3XrVBunChUVFrZV1K5XbIfKdWEiVaVoaAcSKkHCO8gO7yBHS8jRHeRJCZWhQraW92VbeV+27OzH1oq+7KjogZJVrzjFWzZnXRXCYROLUMhC3fXEJnLPA3oGIT51xTQ3F7KzLURvyKysXW/Qxn5Ht0GtRyjesqG4SMTOs74QLQcLOdTUdCYU6pxIoTRIloTpUlhKdU0eFdW1/vOsLLuuiop2D717774tO7vh/y/etoqK2vXKSvtdXp5FeXk+5eX5LT63RMjLg8LCXa+DrKxdQ7xtdbeL7Ho91w0AqoJqbhA67xLfuzf8KgXnl0qhXw3sGfN7D2BtCvODQ+9o0e4DD4aBSTKlbZINdA9C+yQSqV9AQiG70RqqDefk1IpxJhH7gFBll1pmYiGbSKT7bsKem9u8JpVkoWri39BbRDTs3GkVhcJCCwUF8Zd1txUUZOY1EUsqhX42MExE9gbWAKcDZ6YwP6cDkJVlr/f5rVPRazdkZVmtNC8v3ZYkF5Face7VK93WtF8S6V75EHA00FtEVgPXALkAqnqniPQHioGuQERELgGGq+oOEbkIeBGrSt6jqj69h+M4TiuTSK+bMxqJX4+5ZeLFPQc81zzTHMdxnGQg2gbH5hWRTcCnzdy9N7A5ieYkG7evZbh9LcPtaxlt2b69VLVPvIg2KfQtQUSKVXVcuu2oD7evZbh9LcPtaxlt3b76yPC2ZsdxHMeF3nEcJ8PJRKG/K90GNILb1zLcvpbh9rWMtm5fXDLOR++0LiJyLbCvqn4zRcdfDFyoqm+IiAD3YAPofQT8BBsVdf8k5zkYG5Opm2pjA+w4TtsnE2v0TpIRkTNFpFhEykRknYg8LyJHtkbeqnqQqr4R/DwSGw11D1Udr6pvJkPkRWSliBwXk+dnqto5VSIvxgoRaVsD/DkZiwu90yAicilwG/AboB8wGPgLNhJpa7MXsFJVd6Yh72QyCegLDBWRQ1szYxFJ5dfwTltFVdtlAKYAS4HlwBVx4vOBR4L494AhrWjbnsDrwIfAYuDiOGmOBkqAeUG4upXLbyWwMMi7OE68AHcCEeybhjH1HOda4N8xvx8F1gfnNhM4KCZuKuYSKcWGxbgs2N4beAbYjs1vUBxTLtXATuAhoBIIA2XYPAllQfw84Oqg3J8ANgFbgNuD4+8DvBZs2ww8AHQP4v4VnGNFcLyfAUOwAfhygjQDgc+CdFXA94LtPYGPg/NZGywXA+PqlNE5mKvpo2D9nsCGJ6I2xqTtiU3ksxbYBvw3Jm5acK47gnynxPyXLwAbgUXR/wS4JUinwFxsNNmZcf6nSmBZ9FoACoHfB/97CfBWsO1Z4Id17F0AfDmB6+2eqH11rp01Mf/11Obc60m6H+LZ90iMbSuBec25l9pCSLsBzfxTsoMLeCg2VOF8bNiF2DQ/AO4M1k8HHmlF+wYQCCPQJbiJ6tp3NPBMGstwJdC7gfip2HhFNcAE4L160l3LrkL/neCc87E3gXkxceuAicF6j5gyuhF7qOQGYSK17UcrMfHfC/g28FZM+b0LrI65JuYDfwA6AQXAkUHcvpjLJx/ogz2AbqtTFsfF/B7CrkI/A3gSOCwQm03AscBvgVcwofxn8PtGYFbMsXoCK4JlD+ATTKinAl/FHjx5MemfxQSmR1AWRwXbx2Oiezz2Jj4IOCDG/h9js7zFCv0J2ENOg+23AoVx/qcdwMIYG+4A3gjyyAaOCNJ9PfY6AEZhD8+8eNdGnetkUtS+OtfOZS2915N0P+xmX53431NPZYxG7qW2ENqr6+bzSU1UtRqITmoSyzTgvmD9MeDYoDEv5ajqOlWdG6yXYjX7Qa2RdxKZhr0JbVbVt4HuIjKgsZ1U9R5VLVXVKuxGHiUi3YLoEDBcRLqq6rZoGQXbB2Bf9oXUfO/RXgIFwFpVbexL6fFYzfunqrpTVStV9a3ApuWq+rKqVqnqJkzwjkqkEERkT6xt4FtYDbgSuBs4m9oa9lvAL4Lf/8IEMMqJwMuqulVVt2FCD/AS9haTA3wxyGsAcBJwQVA+IVWdEaQ/Dxsv6mVVjajqGlVdEpPPQuyB+Dmq+hL2BgQ2S1w/Va0I4mL/p+3AwSLSTUSysIfAxUEeYVV9J0j3FDZQ4bDgmGdjFajqxspRVWfWtS9BErnXW0xD9gW68XXsrbJd0l6FPt6kJnWF9PM0qlqD1YZaffy7YHauQzDRrMvhIjI/aNw8qFUNs1reSyIyJ5j0pS6DMFHqHfh145XxLohItojcJCIfi8gOrKYD5poBq8FOBT4VkRkicniw/RaspvxS0Eh5RcxhO2FusHgMB/qKyPPA4cCnwX9d166+IvKwiKwJ7Pp3jE2NMRDYGjywo3yKlUU/zN2zXlXXYX73cqAgxhde91odjNWeawLxfAJz54C5nqIPhLrsidVsm8vJwPMQ93+K/q9vYW8GBfHyCuz9D/DN4IFwBvZgawkXicgCEblHRHrEiU/kXk81E4ENqvpRPfGN3Utpp70KfSKTmjRp4pNUICKdgceBS1R1R53ouVgNdhTwZ+C/rWkbMEFVx2A1yAtFZFKdeMFe9yupnQ+4sfI7E6ttHQd0w1wg0WOhqrNVdRomiP/FRIOgZvkTVR0KfAm4VESOFZE8oAhzndRlLuaS24iV34+BwfU0Nt4Y2D5SVbsC32TX66Oh81oL9BSRLjHbBmO+5UT4PB8R2QNzQYwRkfUish44DZgqIr0xQespIvEmDFiFuWHisRMrpyj946SpwdoFYPf/KVrJOA97c6luIK/7gLMw11W5qr5bT7pE+GuQz2jMrff7OGnSfh9jD7SGavON3Utpp70KfSKTmnyeJrj5u9G8V8dmISK5mMg/oKpP1I1X1R2qWhasPwfkBjd7q6Cqa4PlRsz/PL5OktWYX/lqzGd7ALBNRHJF5CQR+W2cw3bBGiu3YMLzm2iEiOSJyFki0k1VQ5hfOBzEnSwi+wavyNHtYezGqcZcC3Xt34E1oEbLL4yJ/k0i0klECkRkQoxdZcB2ERkE/LTO4TZgAhyvnFYB72APi/wgnIeJ5gagc3AOA4L86xJ7rZ6NvVn+GBO30dgslquBM4K3gueBv4hIj6Cso6LxD+Dc4AGYJSKDROSAIG4e9tDLwWrjp8Xk/9Vo3jHusLr/04+C7Vuxa6EYuFVEBga1/8NFJD8oj3exRunf08LavKpuCFxDEeDv7H4NQjomMIoh0I5TsXaTuCRwL6Wd9ir0n09qEtT6Tgem10kzndpX4tOA12Iu9JQSCNY/gA9V9dZ60vSPthmIyHjsv9jSSvZ1itZQRaQT1mi3qE6y6Vjt7g9Yd8rumB94FXAR8d9A7sfcGmuw3jWz6sSfDawM3AUXYDVrgGFYo2YZ1sD6F7W+82dgtdV459A/Zj1aflOxhtfPMIH4RpDkV1hDWwnW2Fn3wXsjcJWIbBeRy+Jkdwb2dvIeVpu/RlVfxspodJDmHMyHXZcXgRMCt8S5mEj+R1XXRwPWEB29Vs/G2iyWYA+OSwBU9X/B/n8IzmMG1kAN8EusZjwfcyc9GJTLFKycIXgoBsT+Tx9iDwqwnjUnADdj//VsTPxvZletuB8YgbnAmk2dNp+vsPs1CInd66nkOGCJqq6OF5ngvZR+0tUK3NKA3dTLMF/ilcG264BTgvUCrAvZcuB/wNBWtO1I7PVyATFdx7Cb7oIgzUVYV7z5mCAe0Yr2DQ3ynR/YEC2/WPsEq8l/jN3041rLviD/IuzB1y1mW9rKD3t1X4eJ8GqsVt8LeBXrNvkq0DNIOw77Yje673eC63A5cG4r2rccezBHr8FoL7SBwHMNXQuN5PUtgt5PLbTvX8G1tQAT7wF17Qt+73avt0b5BdvvjV5zMWlbVH7pCD4EguM4CSMiRdg3CX9R1fvTbY+TGO3VdeM4TisjIidi3xBsIHAPOe0Dr9E7juNkOF6jdxzHyXDa5ABHvXv31iFDhqTbDMdxnHbDnDlzNms9c8a2SaEfMmQIxcXF6TbDcRyn3SAi9Q4T4q4bx3GcDCezhH7NM7DjI/AGZsdxnM9pk66bZhGugjdPg0gVFA6APpOg7yToexR0OxAks55pjuM4iZI5Qp+VByfNg00zYcMM2DgDPguGp8jvBX0mmuj3nQTdR0FWdnrtdRzHaSUyR+hFoNsBFvY939w3Oz+BjTODMANWB8Oz5HaFPkfWCn/PsZCVm177HcdxUkTmCH1dRKDzUAtDv23bylfvKvxrn7Pt2UXQ5whz9xQNbFm+nfeBXl+AnMKWHcdxHCdJZK7Qx6NoDxhypgWAyo2w8U0T/Y0zYeE1JGWo66w86DW+to2g9xGQ27nlx3Ucx2kGbXIIhHHjxmla+tFXl0Co7vwgTUDDULKo9o1h63YBm3AAACAASURBVBzbJtnmHuo7KWgkPhLy4k2m4ziO0zxEZI6qjosX17Fq9I2R181CS+g8BAadbOuhMtj8Tq3wL/0TfPg7QKD7yNo2gr6ToCDuB22O4zgtxoU+leR2hgEnWACoqYAt79W2E3z8d1j2J4vreqAJ/kE/h0571X9Mx3GcJuJC35rkFEK/oy0AhKvNvRNtI1hxL1RugElPptHI9okqrF4NffpAQUG6rXGctoV/RZROsvOgz+Fw0BVwzHMw5CzY8AZEwum2rM0TicDixXDnnXDmmTB4sIWDD4ZVq9JtneO0LbxG35boNxlW3APb50PPMem2pk0RCsH778Obb9aGrcFU7wMGwKRJMHo03HgjHHUUvP467OUeMMcBXOjbFv2OseWG1zq80JeXw3vv1Yr6u+/CzmCa8H33hS9/GSZOtDB0qH02AXDssXDCCbViv/fe6TsHx2kruNC3JYoGQtf9Yf1rcOBl6bam1XnvPXjiCRP24mKrxYvAyJFw7rlWaz/ySKvB18ehh8Krr8Jxx9WK/T77tN45OE5bxIW+jaAKDz0Ex3WfTN9N/4JIqMMMy7BtG/zsZ3D33ZCba2J96aVWW58wAbp3b9rxxoyB117bVeyHDUuN7Y7THkioMVZEpojIUhFZLiJXxIm/QEQWisg8EXlLRIYH24eISEWwfZ6I3JnsE8gEKivh29+Gs86CH994DNSUwZbMn3hFFR57DIYPh3/+E376U/O7v/023HQTfPGLTRf5KKNHm8BXV5vYL12aXNvbGkuXwlVXwfe/D3/4A7zwAnz6qTVaO06jNXoRyQbuAI4HVgOzRWS6qn4Qk+xBVb0zSH8KcCswJYj7WFVHJ9fszGH9evjKV2DWLPjFL2D2W0cDUPzc64w75/D0GpdC1qyBCy+Ep56CQw6BZ5+1mngyGTHCxH7yZBP7116zh0qmsG0bPPII3HefXT/Z2fZg3LKlNk1REey/Pxx44K5h330hLy99tiebSMRcf6pwxBHptqbtkUiNfjywXFVXqGo18DAwLTaBqsaOG9CJpAwYk/nMnWtuigULrGZ7ww3wxLN9WLF1JNuXvsYNN2TeHCqRCPz1ryY2L70Et9wC//tf8kU+ykEHwRtvmK//6KNh0aLU5NNa1NTAc8/BN75hbRX/939QVga/+519R7B5M2zaBDNnwt/+BuefD3372lvSL38Jp51mZVJUBAccYI3aP/853H8/zJ4NFRXpPsPEUbWeWJdfbo3uRxxhrr4pU1r/f54zB6ZOtYfq5ZfDvHlt7N5V1QYDcBpwd8zvs4Hb46S7EPgYWAUMC7YNAXYC7wMzgIkN5HM+UAwUDx48WDOdRx5RLSxUHTxY9f33d42ree9irbq/QPNyKvWii1RralrHptWrVdesSd3xP/hAdcIEVVA97jjV5ctTl1ddlixRHThQtXdv1XnzWi/fZLFwoepll6n272/l16uX6g9/qDpnjmokktgxysos/b//rXrllaqnnqp64IGqOTl2TFAtKlI94wzVp55SraxM7Tk1lyVLVK+5RnX//c3mnBzVqVNV//Uv1d//XrV7d9WsLNXvfU913brU2rJ0qerXv2529OypevzxteV5wAGq115raVoDoFjr09f6IrRWgL8WR+j/3ED6M4H7gvV8oFewPjZ4CHRtLM+xY8emvlTSRDis+stfWslPmKC6YUOcRKueUn0AveOaNxRUv/a11N50FRWqV11Ve4F+4QuqN9+s+tFHyTl+ZaVd8Hl5djPce2/i4pRMPvpIdY89zIa5c1s//6ayaZPqH/+oOmZMraBNm6b65JOqVVXJy6e62sTziSdUv/99e4iACeZ3vqP68suqoVDy8msOn35q1+Qhh5htIqpHH636t79ZOcWyebPqxRdbeXXurPrrX6vu3Jlce9assbLKzraH41VXqW7fbnGbNqneeafZJ2L2HnKI6m9/a+eRKloq9IcDL8b8/jnw8wbSZwEl9cS9AYxrLM9MFfrSUqtFgeq55zYg3lXbVB/MUp1/td5yi6WfPFm1pCT5Ns2YUVszOvts1RtuUB07traGN2KE1Z7mzWueOL/9turw4XasM86o58HWinz8sb1Fde+uOnt2em2JR1WV6n//q/rlL6vm5lq5jRljgr9xY+vYUF2t+txzqt/6lmqXLmZD376qF12k+tZbVllpDdavV/3zn1WPOKL2evzCF1T/8IfE3jyXLVP9yldsvz32UL3//pbbvnWr6uWX29t4bq6Vyfr19adfvdrsHT++9hwmTLDzami/5tBSoc8BVgB7A3nAfOCgOmmGxax/KZoh0AfIDtaHAmuAno3lmYlCv3Kl6siR9kp5660JiObz41RfmqiqdoHm5KiOHp28V9Ft21TPP9+ugL33Vn3xxd3tve021UmTamslQ4ea++Cddxq/YUpKVC+80Pbdc0/VZ59Njt3J4JNPVIcMUe3WTfW995JzzHBYdcEC1WeesZrxQw/Zm8vf/qb6pz+p/u539hC9+moTih//WPUHP1A97zzVb37T3tpOOcVcS6Dar5/qT35ix0wn5eWqjz+uetppqgUFZtvgwao/+5m9FSX7zWzrVtV//MNce1lZtZWNG26wh3RzmDGjtvIydqzqG280/Rg7d6redJNVEERUzzqr6fZ8/LGdx4gRZktWlp3nP/5h591SWiT0tj9TgWWBD/7KYNt1wCnB+h+BxcA84PXogwD4arB9PjAX+FIi+WWa0L/5pmqfPiYsL7yQ4E5zf6b6UK5qqExVVZ9/3l4R99675S6Vxx9XHTDALrSf/MR8tw2xfr3qXXepnnRSbS1zwADV//s/e62vrt41/fTpVoMSUf3Rj1R37GiZvalg5Up7cHXtag+uplJVZfvdfLPqySebAERrbI2F3FxzKfTqZe0Ge+9t/txRo8zf+8wz6XeVxKOkxPzgU6fWuvn239/e+JYsqX+/UMje5BYtMpF99FHVv/5V9frr7fo480zzbR9ySO31tc8+5g5ZtCg5tofD1jax5552/GnTEvOdV1ebG2bAANvvi19MThvPokXWTrLPPrXXxCmnqD74YPP/+xYLfWuHTBL6u++2P3HYsIZvht1Y87zqA6iura1qz5pl4tCnj2pxcdNtWbOm9lV29OjmuS62b1d94AGr4RUV2bF69LDX/EcfrW2YOvhgs7ct89lnqvvua6L75psNpy0rs4fa1VerHnOMvbpHhXv//VW/+13V++6zc37/fWt4Xr5cddUqc7mUlJirLh1tE6lg82Z7WznmmNo3vtGjzac/bZq5W/bbz9pDovHxQteu9sD9whdMRH/yE9X//S915VRervqb35hLKifHGrQ3b949XThsHSaGDdPP3S0zZybfnkjEzvfSS1UHDbIKUnPdSy70aSAUUr3kEivh449vxqtZdanqgzmq71+xy+YlS1T32svE6aWXEjtUOGy1kq5d7fX7ppt2r4U3h/Jy8yd/61sm9mANrr/+dXIbC1PJ6tUmSJ062St+lC1brOfJT35i/tVoDTYry3zmF19sb0bpbnNoC6xZY26+ww6zmu+IEdam9PWvm/vummtUb79d9eGHVV95RXX+fNsnndfI+vWqF1xg/2e3bqq33FL7IH7xxdoG8IMPVn366dZ5QIfD5lZsLi70rcy2baonnmile/HFLXgNf2mC6gvjd9u8Zo3dTLm55gtuiA8/VJ04UT9v0E1WT5q6VFfba/mKFak5fipZu9ZcJ0VF5jM/6KDaGmdenpXfL35h7rNUNIg76WPRInNJRtuqjjrK1ocMsbax1uranAxc6FuRpUuthpibq/r3v7fwYPOust43Vdt3i9q2zRpKwXpk1KWqSvW660yoevRQveeezHEbpIL1660W16WL6pQp1mg2c6Z1PXUyn5desjaS/v3tfmqr3xA0RENC74OaBSxdCjffDOvW2afh+fm7hkS2lZba4Fw5OTaC4sSJLTSq/2RY/GvY9GbtPLQB3bvDiy/apBsXX2x2/+Y39gXorFnw3e/axBzf+Ab88Y/Qr18Lbclw+vWzETNVIcun4+lwHH+8fc2aqXR4oV+zBn71K7jnHigstLFQqqp2D9XVtcuGGDECpk+HIUOSYFzvwyEr34YtriP0YFPmPfqojRlz0002bk6XLnD77TBoEDz9NJy8+25OPYjUjmvvOJlEhxX6rVutBv+nP0E4DBddZIOK9e3b8H6qu4p+7MMgFLKxLvLzk2RkdgH0OQI2vl5/kmwbO2bAALj2WhOqiy6ycXO6dEmSHY7jtGs6nNDv3GnifvPNsGMHnH221egTrYGL1LpqWoV+k2HBL6FqC+T3qtema66BsWPNBXHooa1km+M47YIOI/ShkE1scd115uI45RSr9R58cLota4To9IIbZ8CepzaY1N00juPEI+ObnSIRePhhGxb3Bz+wcbjfesvGQW/zIg/Q81DI6WR+esdxnGaQsUKvarPsjB0LZ5wBnTrZ5BYzZ9qY1e2G7DzoM9EmDHccx2kGGSn0s2bBMcfASSdBSQn8+982QcHUqe20V0W/Y2DHh1CxPt2WOI7TDskooV+82KblO/xw+PBD62a4ZInNxdqu+0b3m2zLDfX3vnEcx6mPjGmM3b4dxo+3j5Wuvx4uuQQ6d063VUmixyGQ282EfsgZ6bbGcZx2RsYIfffu1uh6+OHQu3e6rUkyWdnQ9yj30zuO0yzas0NjN770pQwU+Sj9JkPZx7Dzs3Rb4jhOOyOjhD6jifandz+94zhNxIW+vdD9YMjv7e4bx3GajAt9e0GyoO/RVqNXTbc1juO0I1zo2xP9J0P5KvPVO47jJIgLfXvC+9M7jtMMXOjbE132g8IB7qd3HKdJuNC3J0SsVu9+esdxmoALfXuj32So3GBj3ziO4ySAC317I9qf3octdhwnQVzo2xud94ZOQxqcXtBxHCcWF/r2SL9jAj99JN2WOI7TDnChb4/0mwzV22Db/HRb4jhOO8CFvj3i4944zu5sXwjrX0m3FW0SF/r2SNEg61Pv/ekdx6jcBK8dB29MhR0fpduaNocLfXul32TYOBMiNem2xHHSiyrM/j+o3g5Z+TD3x+m2qM3hQt9e6T8Zakph65x0W+I46WXlg7DqcRh5HYy4FtY+C2ueTbdVbQoX+vZK36Nt6e4bpyNTvgaKL4LeR8ABl8F+P4Su+1utPlyVbuvaDC707ZWCPtB9hDfIOh0XVXjvPIhUw+H32ZSb2Xkw5jYo/QiW/jHdFrYZXOjbM32PgU1vec3F6Zgs/xusexEOuQW67Fu7feAUGHQKLLoeytemz742REJCLyJTRGSpiCwXkSvixF8gIgtFZJ6IvCUiw2Pifh7st1RETkym8R2e/pMhXAFb/pduSxyndSn9GN6/DPofD8P+b/f4MbdaTX/ebnLVIWlU6EUkG7gDOAkYDpwRK+QBD6rqCFUdDfwWuDXYdzhwOnAQMAX4S3A8Jxn0PcpmnkqGn76mAuZfCZ892vJjOU4qiYRh1jkgOXDYPTaqa1267AMHXgYr/wWb3mkdu1Y9aQ+Wmp2tk18TSKRGPx5YrqorVLUaeBiYFptAVXfE/OwERMfQnQY8rKpVqvoJsDw4npMM8rpDj0NaLvTbF8OLh8Li38DbZ8KGGcmxz3FSwZJbYdPbMO7PULRH/emG/xwKB8GcH9nDIZVsehve/gZ8cDO8MA62LUhtfk0kEaEfBKyK+b062LYLInKhiHyM1eh/1JR9g/3PF5FiESnetGlTIrY7YP3pN8+CmvKm76sKH/0NXhwHVZvgyMfM1/nWV6Hsk+Tb6jgtZfsiWHAV7PEVGPLNhtPmdjb//dY5sOKfqbNp52fw5qlQtBdMfBxCJfDieFh2R5uZNyIRoY/zXsRu1qvqHaq6D3A5cFVT9g32v0tVx6nquD59+iRglgPYcAiRatjcxNfT6u3w1tdh9gXQZxKctAAGfxUmPWW1n5nTIFSWGpsdpzmEq+Hdb0FuNxj/t/gum7rsdTr0ORLm/9yu+WRTUw4zv2yuz6Omw56nwknzof+x1u3zzVOhamvy820iOQmkWQ3sGfN7D6ChpuyHgb82c1+nqfQ50nyVG16H/sclts+md+GdM6wP8uibzZcpwTO/635w5CPwxkl2U018rDbOqZ+anTDvF5DfG0b8Mj02LL7R/MQtYc8vw0G/SI49yWbxr2Hb+zDxSetenAgi5uJ5YSwsvBbG3pY8e1Rh1rmwbR4c9TR0O9C2F/Sx30v/CPMuh+dHwxEPQt8jk5d3E0nkDp4NDBORvUUkD2tcnR6bQESGxfz8IhAdbGI6cLqI5IvI3sAwwLuIJJPcLtBrfGITkUTC5od/ZSKQBce/BcN/truQDzgBDvk9rH4SFv4qJWZnFNvmmZAs+xMsvNrcYa3N8rtgfiDQ+b2bFzRsDfKb3m59+xtjy2y7dvf+lj2MmkKP0bDP+bDsdmuPShaLfwOf/QdG3wSDvrhrnGTBAT+GE96FrDx49ShYeH3q2wrqQ1UbDcBUYBnwMXBlsO064JRg/Y/AYmAe8DpwUMy+Vwb7LQVOSiS/sWPHqtME5l2p+mC2avWO+tOUr1V95VjVB1B98xuqVdsbPmYkovruuZb+0/8k195MIRJRXfJn1YfyVJ8YqLruZdXXpqg+mKO6YUbr2bFhhuX52omq4VDzjxMqU31yD9XnxqiGa5JnX0sJlas+fYDZVrWteceo2KT6aA+7ByKRltu06r92b7x9VuPHqy6xdA+g+vLRqjtXtzz/OADFWp+G1xeRzuBC30TWvWoX0epn48eveU71sT6qDxeqLr878Qu9plL1xcNtvy1zk2dvJlC5WXXGNCv317+oWrHRtldtU316f9XHequWfpJ6O8pWWl7T92u+CMay8mE7p4/uavmxkkXxJWbTupdbdpylt9txPnu8ZcfZtlD1kc6qzx9qD6FEiERUP75X9ZFOqo/1Ul39dMtsiENDQu/O10yg9+E2al/dbpbhaph7mQ3dWtgfphTDPucl1ogFkJ0PE5+A/F7WOFuxIfm2RylfAzNPhZePNF/32hchVJq6/FrCxpnmd137HIz5g/ljoz7jvO4waTpEQqlv0K7ZCTOmWV5HTbe8W8rgr0PfSeYGqt7W8uO1lA1vwNLbYNiFibdB1ce+37dhQ+Zeao2nzaFqC8w4xVymk56EnMLE9hOBoefAlDlQtCfM+BLMuaTVvmp3oc8EcgpN7GPHvSldDi9PgCW/h2E/gBPeg251v3NLgML+1hOnarN1uwxXJ8/uKGuegedH2efskRB8eAu8MQUe6wEvjLeH1erp6ReeSNjaLF49BrIL4YRZcMAluz84u+4HEx6BkkUw69upmfJRI/DuOVCyECY8ZAN5JQMRGPsnqN4KC65NzjGbS6jUyq/zvnDIzS0/XlaOndvOT+0aayqRELz1NahYaw3CRXF7ijdM1/3tutnvR9ZY+9LhsGNZ04/TRFzoM4V+k61HQtVWG7b1+TEm9hOfgEPvSLzmEY+eY+Cwf1ojXfEPktc3OFxltZoZX4LCPay2c+J7cNo2mPyyffCSU2iNaDOnwWO94LlRUPwj+OwxqNyYHDsSoXw1vHas9dzY6yyzteeY+tMPPBFG32LD5y66Pvn2LPq1HXv0b2HgSck9do9RsO8F8NEd1m89Xcy9FMpX2YBlOZ2Sc8x+R9tbywc3muA3hTk/tsrUF/4Ovb/QfBuy82HcH+3Nr/wzeGEMrLi/+cdLhPp8OukM7qNvBhveNP/j84fa8qUJqmWfJjePeVfasZf8qeXHKlmm+twhdrzZP1Stqag/bU2FNTguuE711eNUHy6y/R7AGune+77qJw+o7lzVcrviseop1Ud7mn/14/sS3y8SUX3nnKBB+7Hk2fPZ43bMd76VnIbFeFRutnN+ZXLq8miI1c/YOb5/efKPXfaptTu9+bXE91l2p9kz96fJtWXnKtWXJwUNu99suENFI9CAj160jXy5Fcu4ceO0uLg43Wa0L8LV5uoIV8DBV8HBV9urajLRiH0AsuYZOOaF5vtMP/kXzP6BdTs77B7YY1rj+8QSCdnXjhtnwsYZNoJnKBiFo/NQ8zH3PcqWnfZOvE2iLuFKeP9nsOzPNtTEhIfNLdPUY7xyDGxfACe8Y7XllrBtPrx0hPmaj3sDsgtadryGWPYXKL7Qvpge/NXU5VOXqi3w7MHW7nHibKsBJ5uF11tX2GNfq52DuT42zoRXj7UB1I562oZDTiaRMCy+ARb9ytxUJ81t1huMiMxR1XFx41zoM4i1L0BuV+hzROryCJWaX7FiLZz4v12Hh01k39kX2kBTfSfBEQ80PFZJokTCJqQbZ9hNuWmmiQXY8ftMqhX/rvsnJvw7lsLbp1sf+f0vsb7SzRWcinXwwqEg2TBlNhT0bd5xKjfZmESRkDWsFw5o3nESJVJj3wdUb4eTP4ScotTmF+Wt02H1E3Z99RidmjxqKuDZ4daoOmVu/ZWispVW5vm9rJ0rr1tq7IHg2n2r2R+sudA7yaVshQlXQT84cZY9XBpj61wTzrKP7W3joKuSXzOKohEo+bBW+DfOgMr1FlfQd1fh737wrh+MqcIn99nn69kFcNi9MOjkltu0pdg+VOt5KEx+xSbIaArhanj9eBuS+riZ0OvQltuUCBtmwKtH2xR9I65JfX6fPmLXychfw8FXpjavVU/Am1+FcbfDfhfuHh8qsw4NOz+ztqOmvs21Mi70TvLZ8Dq8djwMmGK9cuoTbdXgU/CfQX5fmPCgiWxromoN05tmmnBtmlnbEJfXA/pMNJt6Hw4f/QVWPmBTNR7x7+b1rKiPlQ/BO2fCPt9LfKyWqP2zL7CvX494AIacmTybEuGt02HNU3DyEui0V+ryqVhnLpsuw+yr7WS7Huuiatfwtrlw8jIo6B0TF4E3T7PzPvp5+1q8jdOQ0Ke94TVe8MbYdsLSO4IGsyvix1dstI+JHkD1jVOsga+tULZSdcX9qrPOU50+rLZx98Es1YXXp+7L0Pd/bvksvT3xfRor51RT9pk1Xs48LXV5VJdYB4KHC1VLlqQun7psW2Rflb93wa7b519jZf7hH1rPlhaCfxnrpIRIRPW98+2G+OTBXePWv6b6xAAbHmDJn9PTc6MplK+1oR5S/QVwJKz6xpdMXNa92nj69a9Z2tdPTu+wBAuvD75OTcDmplK1TfWFw+w80zHcRvHFqg9I7X//6aN2ru9+u+1ftzE0JPTuunFaRrgaXjsOts6G4960xrOFv7JeBF33s54qqWpQa6+EdgQN2uutcbbz0PjpylbYuOb5fRNvC0kV4Up4Zrg1yJ40L3lulaqt8PoJ1pg+4RHY8yvJOW5TqN4OTw+DrgfYSJcvTbDeUce+npoePymiIdeNfzDltIzsPJtsoaCffdT06tE2nOzQb9tHRS7yu5Pb1T6WQe1z+nhDPYRKbXgDjdjwBukUebCG6TG3Qsli+OivjadPhMrN8Opk2L7QPuxLh8iDDR0x6kbr8fLyJMjvafa0I5FvDBd6p+UU9LEG2ertNoXaEQ9a//hkfc2YiXTZB458FHYsgXe+ueswCRqBd8+GHR9amqZ0YU0le0yzvuQLrrauni2hYoMNJVG61B56yejZ1BKGngs9x4KGYNJ/beiPDMKF3kkOPUZZDf7kD2DIGem2pn3Q/1gbFG3NdBPPKAuugdVPWVz/Y9NnX11EYOwfoabMpvNrLuVr7c2vbAUc9awNF5FusrLhmBdtdqieY9NtTdJJcf8lp0PR7YB0W9D+2O8i808vvsG+dgVzfe3zXYtra3Q7EPb7oY0oue/3Gx7vJx47V5m7pnI9HPN863e1bYj8XhYyEK/RO046EYFxd9iUkLO+bVPT9TnStjV36IZUM+Iac9cV/7BpA9yVrYRXjoKqjVZ7bksin+G40DtOuok2aOf3tSn9Jj7e9C9nW5O8btZ4ufkdGyk1EUo/NpGv3mZfBqdymA5nN1zoHactUNAXps630NyxcFqTod+GnuPsi+fGJlfZsdREvqbMBhFrreEbnM9xoXectkJedxuSoT0gWdbnvGKtTZJdHyUfwCtHQ6TaRtvseUhrWejE4ELvOE7z6H0Y7H2OzWJWunz3+G0LTOTBRD7a2Oy0Oi70juM0n9E32nzFcy/ddfvWudZPPisPjpvRvGksnaThQu84TvMpHAAjroY1T8Pa523bltk2UUdOZxP5Nj68b0fAhd5xnJax34+gy342/++GGTb2UV4POH6mfQHspB0XesdxWkZ2Hoy9DUqXmbumoJ+JfCrHrneahAu94zgtZ+BJsNfp1uB67BvJmSLSSRo+BILjOMnhiODjqbb6RW8HxoXecZzk4ALfZnHXjeM4TobjQu84jpPhtMmpBEVkE/BpM3fvDWxOojnJxu1rGW5fy3D7WkZbtm8vVe0TL6JNCn1LEJHi+uZNbAu4fS3D7WsZbl/LaOv21Ye7bhzHcTIcF3rHcZwMJxOF/q50G9AIbl/LcPtahtvXMtq6fXHJOB+94ziOsyuZWKN3HMdxYnChdxzHyXDardCLyBQRWSoiy0Xkijjx+SLySBD/nogMaUXb9hSR10XkQxFZLCIXx0lztIiUiMi8IFzdWvYF+a8UkYVB3sVx4kVE/hSU3wIRGdOKtu0fUy7zRGSHiFxSJ02rlp+I3CMiG0VkUcy2niLysoh8FCzjzgMoIucEaT4SkXNa0b5bRGRJ8P89KSLd69m3wWshhfZdKyJrYv7DqfXs2+C9nkL7HomxbaWIzKtn35SXX4tR1XYXgGzgY2AokAfMB4bXSfMD4M5g/XTgkVa0bwAwJljvAiyLY9/RwDNpLMOVQO8G4qcCzwMCHAa8l8b/ej32MUjayg+YBIwBFsVs+y1wRbB+BXBznP16AiuCZY9gvUcr2XcCkBOs3xzPvkSuhRTady1wWQL/f4P3eqrsqxP/e+DqdJVfS0N7rdGPB5ar6gpVrQYeBqbVe4KRHgAAA1VJREFUSTMNuC9Yfww4VqR1Rl1S1XWqOjdYLwU+BAa1Rt5JZBpwvxqzgO4iMiANdhwLfKyqzf1SOimo6kxga53NsdfYfcCX4+x6IvCyqm5V1W3Ay8CU1rBPVV9S1Zrg5ywgbWMH11N+iZDIvd5iGrIv0I2vAw8lO9/Wor0K/SBgVczv1ewupJ+nCS72EqBXq1gXQ+AyOgR4L0704SIyX0SeF5GDWtUwUOAlEZkjIufHiU+kjFuD06n/Bktn+QH0U9V1YA93oG+cNG2lHL+DvaHFo7FrIZVcFLiW7qnH9dUWym8isEFVP6onPp3llxDtVejj1czr9hNNJE1KEZHOwOPAJaq6o070XMwdMQr4M/Df1rQNmKCqY4CTgAtFZFKd+LZQfnnAKcCjcaLTXX6J0hbK8UqgBnigniSNXQup4q/APsBoYB3mHqlL2ssPOIOGa/PpKr+Eaa9CvxrYM+b3HsDa+tKISA7Qjea9OjYLEcnFRP4BVX2ibryq7lDVsmD9OSBXRHq3ln2qujZYbgSexF6RY0mkjFPNScBcVd1QNyLd5RewIerOCpYb46RJazkGjb8nA2dp4FCuSwLXQkpQ1Q2qGlbVCPD3evJNd/nlAKcCj9SXJl3l1xTaq9DPBoaJyN5Bre90YHqdNNOBaA+H04DX6rvQk03g0/sH8KGq3lpPmv7RNgMRGY/9F1tayb5OItIluo412i2qk2w68K2g981hQEnUTdGK1FuTSmf5xRB7jZ0DPBUnzYvACSLSI3BNnBBsSzkiMgW4HDhFVcvrSZPItZAq+2LbfL5ST76J3Oup5DhgiaqujheZzvJrEuluDW5uwHqFLMNa5K8Mtl2HXdQABdgr/3Lgf8DQVrTtSOz1cgEwLwhTgQuAC4I0FwGLsV4Es4AjWtG+oUG+8wMbouUXa58AdwTluxAY18r/bxEm3N1itqWt/LAHzjoghNUyz8PafF4FPgqWPYO044C7Y/b9TnAdLgfObUX7lmP+7eg1GO2FNhB4rqFroZXs+1dwbS3AxHtAXfuC37vd661hX7D93ug1F5O21cuvpcGHQHAcx8lw2qvrxnEcx0kQF3rHcZwMx4XecRwnw3GhdxzHyXBc6B3HcTIcF3rHcZwMx4XecRwnw/l/uad3N8e/wGYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Cross Entropy Loss')\n",
    "pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "# plot accuracy\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Classification Accuracy')\n",
    "pyplot.plot(history.history['acc'], color='blue', label='train')   # 'accuracy'\n",
    "pyplot.plot(history.history['val_acc'], color='orange', label='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check without any dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00392156862745098"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.0/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00392156862745098"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.0/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00392156862745098"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1./255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00392156862745098"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
