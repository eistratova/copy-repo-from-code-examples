# Extract deep learning features from images using simple python interface
https://github.com/chsasank/image_features


# STRATEGIES for anom detection:

1. similarity based techniques : 
     comparing against a set of reference images (without defects) by extracting the image features using DL algorithm & comparing using distance measures 
     https://github.com/microsoft/computervision-recipes/tree/master/scenarios/similarity
     Image similarity: https://github.com/microsoft/computervision-recipes/tree/master/scenarios/similarity
     

2. using auto encoders : reconstruction error of images with defects should be higher compared to normal images
  VAE: https://github.com/tarekmuallim/Anomaly-Detection-using-Variational-Autoencoders    
   AE: https://www.pyimagesearch.com/2020/03/02/anomaly-detection-with-keras-tensorflow-and-deep-learning/  
     
     
3. standard anomaly detection algos: 
extracting image features using DL algo & passing through trained ML methods line one-class SVM, or other anomaly detection techniques (should have 2 classes: good vs others)

    https://machinelearningmastery.com/model-based-outlier-detection-and-removal-in-python/
     1. Isolation forest (contamination = 0.1), removes outliers(few instances or abnormal values from dataset) will be removed.
     2. Minimum covariance determinant: (Hypersphere - ellipsoid - elliptic_envelope(contamination = 0.01))
     3. Local outlier factor (Far from feature space) - works for few features - (contamination = 0.1)
     4. One class SVM (nu = 0.01)  - setting nu is trial and error method
     
     https://towardsdatascience.com/similar-images-recommendations-using-fastai-and-annoy-16d6ceb3b809 - Try 2nd*
     Fastai
     pytorch's hook - In order to get the output of an intermediate layer from a model in PyTorch, we use a functionality called Hook. 
          Hooks can be added to extract intermediate values of a model from either the forward pass or the backward pass. 
          "Hook - functionality to save the embeddings in 2nd last layer of our trained model"
          Spotify's Annoy (Approximate nearest neighbour oh yeah)- Using binary trees for ANN
     
     https://blog.usejournal.com/fastai-image-similarity-search-pytorch-hooks-spotifys-annoy-9161bf517aaf
     - Hook & Annoy - Code (Try 2nd*)

     
     https://becominghuman.ai/extract-a-feature-vector-for-any-image-with-pytorch-9717561d1d4c
     Uses feature embeddings*  - pytorch
     "If you plan to use images that are very different from ImageNet, you may benefit in using an earlier layer or fine-tuning the model."
     Cosine similarity between two images. How to run for entire dataset?
     
     
     https://forums.fast.ai/t/extracting-features-from-learn-model-resnext50/15063
     Above method implementation only in fastai - Expected runtime error - (check if code is run on cpu, or gpu  and change unfreeze accordingly)
     
     https://www.kaggle.com/jtbontinck/cnn-features-extraction-xgb-submission
     Code for feature embeddings + XGB
     
     
     https://datascience.stackexchange.com/questions/25122/pre-trained-cnn-for-feature-extraction   (extracting features from cnn in keras)
     https://forums.fast.ai/t/how-to-remove-the-last-fully-connected-layer-from-a-cnn-in-fastai-to-stack-more-than-one-cnn-into-a-fc-nn/26438
     

     Anomaly detection using isolation forest:
     https://www.pyimagesearch.com/2020/01/20/intro-to-anomaly-detection-with-opencv-computer-vision-and-scikit-learn/
     https://medium.com/swlh/algorithms-to-detect-anomalies-in-images-56a1793eba56 - Includes Isolation forest(Pt.2) & Keras Anomaly det method
     Uses isolation forest (ml ensemble method) – based on color, hue and saturation bins – cant detect small differences in images
     works on color channel of the images. Train on green colour images (forest) and test on blue or red images(sea, street) - flags as anomaly


4. siamese networks & triplet loss
     "building image pairs for siamese networks with python " on pyimagesearch 
     
5. GANs



####################################################################
# anom detection using AE:

1. VAE vs AE?
https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf 
     An autoencoder accepts input, compresses it, and then recreates the original input.
     A variational autoencoder assumes that the source data has some sort of underlying probability distribution (such as Gaussian) and then attempts to find the parameters of the distribution. It provides a probabilistic manner for describing an observation in latent space. Thus, rather than building an encoder which outputs a single value to describe each latent state attribute, we'll formulate our encoder to describe a probability distribution for each latent attribute. 


2. Metrics used for Autoencoders? - Losses
VAE loss- KL loss(Encoder) + Reconstruction loss(decoder) (Variational Auto Encoders)
MSE loss (Autoencoders)
Kernel density estimation(In bottleneck) - Adverserial Autoencoder  (https://medium.com/@judewells/image-anomaly-detection-novelty-detection-using-convolutional-auto-encoders-in-keras-1c31321c10f2)


3. Intermediate and latent space in A.E?

An autoencoder reduce an input of many dimensions, to a vector space of less dimension, 
then it recompute the lossed dimension from that limited number of intermediate vectors. 
This intermediate dimension is called the latent space.

In code,
intermediate_dim = 256 - Bottle neck to 256  or 256 neurons/classes/features
latent_dim = 2 - Latent variable is 2D (Need clarity)


4. Color channels? - RGB (Codes for color image (3 dimensions))


5. Types of A.E:  
https://towardsdatascience.com/deep-inside-autoencoders-7e41f319999f 
https://blog.keras.io/building-autoencoders-in-keras.html



Step1: 
VAE: https://github.com/tarekmuallim/Anomaly-Detection-using-Variational-Autoencoders  - Reconstruction loss too high 
Image Tiling: https://stackoverflow.com/questions/28496848/convert-and-crop-image-in-tiles-with-python 
Image similarity: https://github.com/microsoft/computervision-recipes/tree/master/scenarios/similarity  
AE: https://www.pyimagesearch.com/2020/03/02/anomaly-detection-with-keras-tensorflow-and-deep-learning/ 

Step 2:
https://www.pyimagesearch.com/2020/01/20/intro-to-anomaly-detection-with-opencv-computer-vision-and-scikit-learn/
https://paperswithcode.com/task/anomaly-detection/latest
https://github.com/hoya012/awesome-anomaly-detection 
https://medium.com/swlh/algorithms-to-detect-anomalies-in-images-56a1793eba56

http://www.innovationatiris.com/iris-innovation/files/2019/11/NatasaSDj_DiscoveryScience2019.pdf

####################################################################################




# points:
1.  VAE based method doesn't work for highly similar images with small differences (Not much distinguishable features - Reconstruction loss too high), 
     it works well for distinctly different classes
2. Image similarity: comparison using distance measures (between 0 and 1)
3. Extract features using CNN & use standard classifiers like SVMs





REf:
An Attention-Based Network for Textured Surface Anomaly Detection - Appl. Sci. 2020, 10, 6215; doi:10.3390/app10186215
Classification is a Strong Baseline for Deep Metric Learning - Andrew Zhai , Hao-Yu Wu
Anomaly detection with convolutional neural networks for industrial surface inspection - Benjamin Staar, Michael Lütjen, Michael Freitag

https://www.sciencedirect.com/science/article/pii/S1474034620300744   ( Conv layers in AE)
https://ieeexplore.ieee.org/abstract/document/8354254?casa_token=gKlOvvP6vGgAAAAA:nqOAdmEHSwbMvkuRLFXuT8HQBYxuD02DJB1PoVMJ7sYS70JdwwiCop2BIkVaIDfqGTZCbtb3XxY  
https://www.sciencedirect.com/science/article/pii/S2212827119302409  -> triplet loss, may be more computationally intensive – need to check

https://towardsdatascience.com/a-compact-cnn-for-weakly-supervised-textured-surface-anomaly-detection-2572c3a65b80
https://medium.com/swlh/how-to-detect-defects-on-images-16d6cf3ddc1a




