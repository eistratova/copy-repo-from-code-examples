https://blog.roboflow.ai/getting-started-with-cvat/
https://www.datasetlist.com/tools/ : website compiling different tools )

Tips on data annotation:
 Label all the way around the object in question
 Label occluded objects entirely
 Avoid too much space around the object in question
 
 
possible data annotation formats:
1. coco format as json file (see pycoco website); coco format additionally requires label map file (name → id mapping)
2. xml format
3. text file, with id and bbox coordinates
create train, val & test sets with appropriate folder structure for images & annotations.


Training or transfer learning
1. transfer learning- if the class to be trained on, is similar to one of the classes for the pre-trained model. Requires less images
2. Training - if the class is dissimilar to the original training set. Requires more data (heavy augmentation)

#####################################################

###   yolo training  ###

# clone the repo
# set GPU card config in makefile (line no. 27-50)
# install using make

# prepare the dataset & the annotations ( .txt file in the same directory with the same name. 
# Each text file contains the class id and coordinates of a bounding box for every object as shown below
# <object-class> <x_center> <y_center> <width> <height> 
# <x_center> <y_center> <width> <height> are float values relative to width and height of image,
# it can be equal from (0.0 to 1.0])
# if the annotations in the source data are not in yolo format, convert to that format using https://github.com/ssaru/convert2Yolo

# create configuration file (.cfg) & set model parameters here
# copy yolo4-custom.cfg now - rename the copied file to yolo-obj.cfg

# create the following files describing the data
# obj.names, obj.data, train.txt, validate.txt, test.txt

# download the pre-trained weights "yolov4.conv.137" for the convolutional layers and put in the directory build\darknet\x64

# After training is complete weights will be saved as yolo-obj_last.weights for every 100 iterations and 
# saved as yolo-obj_xxxx.weights for every 1000 iterations in the directory build\darknet\x64\backup

# test predictions

# saving intermediate checkpoints: intermediate weights saved in folder: build/darknet/x64/backup/

https://medium.com/ai-world/how-to-train-yolov4-for-custom-objects-detection-in-google-colab-1e934b8ef685
https://medium.com/analytics-vidhya/implementing-yolov4-to-detect-custom-objects-using-google-colab-6691c98b15ff



#####    check if localization algo works with empty annotation file - yes. #####
Yolo requires empty .txt files. 
We need to add negative images, if there are certain classes which are present & similar to our classes of interest, but which we don't want to detect. 
For defective parts, the images should already be having the good part of the sheet in the background, & so we will not require separate good images for training.



#### Saving predicted box information in text files:  #####
Save the paths of test images in a file & send it to the following code - I did not get segmentation fault with any of the following:

OPTION 1: the output is as json file, & coordinates are in fraction format, & centre_x, centre_y, width, height
!./darknet detector test ./build/darknet/x64/data/obj.data ./build/darknet/x64/cfg/yolo-obj.cfg \
  ./build/darknet/x64/backup/yolov4_60_20_20/yolo-obj_last.weights \
  -ext_output -dont_show -out result.json < ./infer_image_list.txt


OPTION 2: output is text file; it has pixel coordinates, but also additional info in the file
!./darknet detector test ./build/darknet/x64/data/obj.data ./build/darknet/x64/cfg/yolo-obj.cfg \
  ./build/darknet/x64/backup/yolov4_60_20_20/yolo-obj_last.weights \
  -dont_show -ext_output < ./infer_image_list.txt > pred_coord2.txt
  
  
  
######  Editing darknet Makefile from colab  ####
!cat Makefile   # see contents of makefile
!sed -i 's/ARCH= -gencode arch=compute_37,code=sm_37/#ARCH= -gencode arch=compute_37,code=sm_37/g' Makefile
!sed -i 's/#ARCH= -gencode arch=compute_75,/ARCH= -gencode arch=compute_75,/g' Makefile  # making changes
!cat Makefile   # verify changes were made


########  Other issues  #######

https://stackoverflow.com/questions/54374935/how-to-fix-this-strange-error-runtimeerror-cuda-error-out-of-memory
Changed a few parameters : batch size, width and height 
https://blog.roboflow.ai/training-yolov4-on-a-custom-dataset/   lower no. of subdivisions & max_batches


1. How to assess localization algo has trained sufficiently – is there an optimal mAP ? 
  Right now, we are stopping when there are no changes in mAP for a few cycles.
  can look at loss <= half of starting loss

2. Generally, we use the val set to tune model parameters & find optimal model – is something similar being done for localization? 
  Which parameters are we tuning for localization?
https://medium.com/hackernoon/efficient-implementation-of-mobilenet-and-yolo-object-detection-algorithms-for-image-annotation-717e867fa27d 

3. If we are not using the val set to tune for alternate models & only using for testing, do we need a separate test set? 

4. We should also be checking recall, not just precision (as in pycoco tool)




####  Hyperparameter tuning - tiny YOLOv4 (darknet)  #####

yolo hyperparameters:

data augmentation: 
mosaic, mosaic bound, blur, angle
random (=1 will increase precision by training Yolo for different resolutions; If you encounter an error out of memory just increase the subdivisions parameter to a higher value)
saturation, exposure, hue, resize
https://blog.roboflow.com/yolov4-data-augmentation/       
https://blog.roboflow.com/why-preprocess-augment/ 


training:   image size/ network size: width=416 height=416 or any value multiple of 32
            batch (ie, batch size =no. of images per iteration, eg 64);     1 Epoch = images_in_train_txt / batch iterations
            subdivisions (no. of pieces the batch is broken into, for GPU memory, eg 16);  
             classes  (change in each of 3 yolo layers) 
             learning rate (optimal value?); (step decay learning rate scheduling strategy? https://medium.com/@riteshkanjee/yolov4-superior-faster-more-accurate-object-detection-7e8194bf1872)
               momentum (??)
               decay  (??)
               filters = (classes + 5)x3 in the 3 [convolutional] before each [yolo] layer, keep in mind that it only has to be the last [convolutional] before each of the [yolo] layers. 
               max_batches = 2000 x #classes  , but not less than #training images;   
               steps = 80% and 90% of max_batches
               IOU threshold,    NMS threshold

[ learning rate: too small → too slow convergence during training; too big → unstable training & oscillations in train/test accuracy.
         fixed lr + momentum : helps in faster convergence, for a given fixed learning rate  : (use large momentum & small decay)
         learning rate schedule: reduces learning rate using a decay parameter, or a patience parameter (no. of epochs to wait before changing LR, when no change is occurring in the monitored metric eg., loss)
         adaptive learning rate: The performance of the model on the training dataset can be monitored by the learning algorithm and the learning rate can be adjusted in response. (RMSprop, Adam)
          https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/  ]


# default data aug in yolo:
Random=0
saturation=1.5
exposure=1.5
hue=0.1
resize=1.5

# edited aug
mosaic=1
random=1
flip=1
angle=30
mixup=1
policy=sgdr



##################################################################





