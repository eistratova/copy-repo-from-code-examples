1. adding different data augmentation options leads to some improvement

2. bs = 16 terminated code on colab, so working with batch_size = 8

3. increasing steps also leads to some improvement - testing this out


4. will try with different learning rate schedules & optimisers
https://github.com/tensorflow/models/blob/master/research/object_detection/protos/optimizer.proto 
https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/  (use large momentum & small decay)

-> Try RMSProp or Adam optimisers.

Sample config file with rmsprop https://stackoverflow.com/questions/47462962/tensorflow-object-detection-api-with-mobilenets-overfits-custom-multiclass-datas 
https://github.com/tensorflow/models/blob/master/research/object_detection/configs/tf2/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8.config 

optimisers:
https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a 
https://ruder.io/optimizing-gradient-descent/
https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1 
https://medium.com/syncedreview/iclr-2019-fast-as-adam-good-as-sgd-new-optimizer-has-both-78e37e8f9a34  

learning rate:
https://kiranscaria.github.io/general/2019/08/16/learning-rate-schedules.html 
https://www.jeremyjordan.me/nn-learning-rate/ 
https://cs231n.github.io/neural-networks-3/#baby 
